{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c853c023",
   "metadata": {},
   "source": [
    "**02452** *Machine Learning*, Technical University of Denmark\n",
    "\n",
    "- This Jupyter notebook contains exercises where you fill in missing code related to the lecture topic. *First*, try solving each task yourself. *Then* use the provided solution (an HTML file you can open in any web browser) as inspiration if needed. If you get stuck, ask a TA for help.\n",
    "\n",
    "- Some tasks may be difficult or time-consuming - using the solution file or TA support is expected and perfectly fine, as long as you stay active and reflect on the solution.\n",
    "\n",
    "- You are not expected to finish everything during the session. Prepare by looking at the exercises *before* the class, consult the TAs *during* class, and complete the remaining parts *at home*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35636165",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158ad19",
   "metadata": {},
   "source": [
    "# Week 1: Introduction, data and visualization\n",
    "\n",
    "\n",
    "**Content:**\n",
    "\n",
    "- Part 1: Loading data in Python - the Iris flower data set\n",
    "- Part 2: Cleaning up data in Python\n",
    "- Part 3: Text-representation in Python\n",
    "\n",
    "\n",
    "**Objectives:**\n",
    "- Be able to import data into Python and represent the it in the course format of $\\boldsymbol{X}, \\boldsymbol{y}$.\n",
    "- Be able to do common preprocessing steps for datasets.\n",
    "- Understand the bag of words representation for text documents including filtering methods based on removal of stop words and stemming.\n",
    "- Understand some of the many ways data can be visualized including histograms, boxplots, and scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e82d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d026b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise we will take a closer look on ways to load data in Python and explore some of the many data visualization techniques we can apply to better understand the content of a dataset. \n",
    "\n",
    "We will use a standard data representation throughout this course, by collecting $N$ individual data points $\\boldsymbol{x}_i = \\left[x_1, x_2, \\dots x_M\\right]^\\top$ with $M$ attributes/features in a data matrix $\\boldsymbol{X}$ of size $N\\times M$. This means that the $i$'th row in $\\boldsymbol{X}$ corresponds to the $i$'th data point, while the $j$'th column contains all observations of the $j$'th attribute. As discussed in the lecture, these attributes can be discrete or continuous and of the types nominal, ordinal, interval or ratio. \n",
    "\n",
    "Later in the course we will use the data matrix $\\boldsymbol{X}$ for solving both 1) **predictive tasks** where we aim to predict unknown values of some target attribute and 2) **descriptive tasks** where we aim at finding human-interpretable patterns that describe the data.\n",
    "\n",
    "- Using $\\boldsymbol{X}$ for predictive tasks is what we call **supervised learning** and consists of learning a mapping between the attributes in $\\boldsymbol{X}$ and a **target attribute**. We will here call $\\boldsymbol{X}$ the **input data** and denote the target attribute by $\\boldsymbol{y}=\\left[y_1, y_2, \\dots y_N\\right]^\\top$ such that there is a target value associated to each $i$'th input data point. We can think of supervised learning as finding a function, $f$, such that our predictions $\\hat{\\boldsymbol{y}} = f\\left(\\boldsymbol{X}\\right)$ are \"close\" to the true values in $\\boldsymbol{y}$. If the target attribute is discrete we solve a **classification task** while we solve a **regression task** if the target attribute is continues.\n",
    "\n",
    "- Using $\\boldsymbol{X}$ for descriptive tasks is known as **unsupervised learning** and is used for exploratory analyses of the data. Examples of unsupervised learning techniques are 1) clustering techniques that relates to finding hidden group structures $\\boldsymbol{X}$, 2) **anomaly detection** that considers finding abnormal data points or 3) **representation learning** where we seek some meaningful latent representations of the data.\n",
    "\n",
    "In the following exercise, we will get familiar with loading datasets distributed as various types of files into the $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$-format described above. We will generally load data into Pandas dataframes as the Pandas library provides flexible tools for data analysis, except when working with more complex data types as text and images.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120a89a",
   "metadata": {},
   "source": [
    "## Part 1: Loading data in Python - the Iris flower dataset\n",
    "\n",
    "We consider the Iris flower dataset (downloaded [here](http://archive.ics.uci.edu/ml/datasets/Iris)) - or Fisher's Iris dataset - is a multivariate dataset introduced by Sir Ronald Aylmer Fisher (1936) for the problem of classifying Iris flower types.It is sometimes called Anderson's Iris dataset because Edgar Anderson collected the data to quantify the geographic variation of Iris flowers in the GaspÃ© Peninsula. The dataset consists of $N=50$ samples from each of $C=3$ species of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). Each observation has $M=4$ attributes measured: the length and the width of sepal and petal, in centimetres, hence $\\boldsymbol{x}_i = (x_1,x_2,x_3,x_4)$. Based on the combination of the four variables, Fisher developed a model to distinguish the species from each other - it is used as a typical test for many other classification techniques (see [here](http://en.wikipedia.org/wiki/Iris_flower_data_set)).\n",
    "\n",
    "A simple format of storing data is the comma-separated values-file format (or CSV). In such files, a sample or an observation is a line in a text document and the document then has as many lines (or rows) as there are samples, i.e. $N$. The attribute values for an observation is written within one line, separated by (usually) a comma or a tab-character in a consistent order. This order is usually defined in a header (the first line of the file), which has a designation of the variable name in some format.\n",
    "\n",
    "**Task 1.1:** Inspect the `iris.csv` file from the associated data folder. Load the CSV-file using Pandas and split it into the standard $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$-format. Make sure to keep $\\boldsymbol{X}$ and $\\boldsymbol{y}$ as Pandas data types.\n",
    "> *Hint:* Open the CSV-file using e.g. Notepad for Windows or TextEdit for MacOS.\n",
    "\n",
    "> *Hint:* Use `df = pd.read_csv()` to load the CSV-file into a Pandas dataframe. Then split it into $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$, e.g. using `df.drop()` and `df[target_attr_name]`.\n",
    "\n",
    "> *Hint:* The class label (the flower species) are stored as text (or strings). Convert it into a categorical attribute using `pd.Categorical()`. Type `help(pd.Categorical)` if you get lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13e1a8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03249d51e994e76f2255ec4e4cf2cea8",
     "grade": false,
     "grade_id": "cell-01438ae71e61428a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the data\n",
    "N, M = X.shape\n",
    "assert N == 150, \"There should be 150 samples in the Iris dataset.\"\n",
    "assert M == 4, \"There should be 4 features in the Iris dataset.\"\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158bfcd9",
   "metadata": {},
   "source": [
    "Sometimes datasets are distributed as Excel-files `.xls(x)`. \n",
    "\n",
    "**Task 1.2:** Load the same Iris data, when it has been stored as an Excel-file using Pandas. Split it into the standard $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$-format as before.\n",
    "> *Hint:* Open `iris.xls` in the data folder to have a look at the file.\n",
    "\n",
    "> *Hint:* Use `df = pd.read_excel()` to load an Excel-file into a Pandas dataframe. For furhter information, type `help(pd.read_excel)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6af0c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18cb762d34e108781908a1ea074abdcb",
     "grade": false,
     "grade_id": "cell-9500efbf0fbe4b07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the data\n",
    "N, M = X.shape\n",
    "assert N == 150, \"There should be 150 samples in the Iris dataset.\"\n",
    "assert M == 4, \"There should be 4 features in the Iris dataset.\"\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac0b8e",
   "metadata": {},
   "source": [
    "Other times data is stored as MATLAB files (`.mat`). \n",
    "\n",
    "**Task 1.3:** Load the same Iris data, when it has been stored in the MATLAB-file `iris.mat`. \n",
    "> *Hint:* Use `data = scipy.io.loadmat()` to load the data file. Check the data structure, e.g. using `data.keys()`, to see what information it contains.\n",
    "\n",
    "**Task 1.4:** Split it into the standard $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$-format and convert $\\boldsymbol{X}$ and $\\boldsymbol{y}$ to Pandas datatypes.\n",
    "> *Hint:* In MATLAB-files, strings are stored in Numpy arrays. To extract the string information, use a list comprehension on the form: `[val.item() for val in data['key_in_the_dictionary'].flatten()]`\n",
    "\n",
    "> *Hint:* Use `pd.Dataframe()` to construct a Pandas dataframe from a `np.Array`. Type `help(pd.DataFrame)` to figure out how to name the columns.\n",
    "\n",
    "> *Hint:* You should construct $\\boldsymbol{y}$ as a `pd.Categorical` with the class names as the values. The information is available in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df1534",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f2aede37bb0cb270e1ab71b279a2a37",
     "grade": false,
     "grade_id": "cell-0b9c7aa26cf36362",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the data\n",
    "N, M = X.shape\n",
    "assert N == 150, \"There should be 150 samples in the Iris dataset.\"\n",
    "assert M == 4, \"There should be 4 features in the Iris dataset.\"\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327099c2",
   "metadata": {},
   "source": [
    "For some modeling tasks, working with Pandas dataframes is cumbersome. Luckily, we can easily convert Pandas datatypes into numerical of type `np.Array` if we stored the data correctly. In the following cell, we do so for the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y to numpy arrays\n",
    "X_numpy, y_numpy = X.values, y.codes\n",
    "\n",
    "# Print the first 5 samples of X and y\n",
    "print(f\"X: {X_numpy[:5, :]}\")\n",
    "print(f\"\\ny: {y_numpy[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232e3b7",
   "metadata": {},
   "source": [
    "In the examples up until now, we have handled the data in the Iris dataset as if to solve a classification problem. We could say that the **primary machine learning modelling aim** is to classify the species of Iris flower based on the petal and sepal dimensions. However, we could also use the dataset to illustrate how to do regression without needing to use a whole different dataset. We would achieve this by e.g. trying to predict either of the petal (or sepal) dimensions based on the remaining dimensions, for instance. This changes how we define our $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$-format.\n",
    "\n",
    "**Task 1.5:** Cast the Iris dataset into a regression problem. To do so, set up the $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$-format in Pandas such that we are predicting the petal lengths from the other continuous attributes.\n",
    "> *Hint:* Use `df.drop()` to remove the target attribute and non-continuous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b248254",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb83f027e7cc7d3d90b77ca3230df7c3",
     "grade": false,
     "grade_id": "cell-aa3261e959736cd0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the regression data\n",
    "N_reg, M_reg = X_regression.shape\n",
    "assert N_reg == 150, \"There should be 150 samples in the Iris dataset.\"\n",
    "assert M_reg == 3, \"There should be 3 features in the Iris regression dataset.\"\n",
    "\n",
    "# Display the first few rows of the regression dataframe\n",
    "X_regression.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ea1af",
   "metadata": {},
   "source": [
    "#### Basic plotting in Python\n",
    "\n",
    "In the following we will do an initial data analysis of the Iris dataset through basic plots of the attributes. We will recreate the plots in section 7.1 of the course book.\n",
    "\n",
    "**Task 1.8:** Plot histograms of the four attributes using `plt.subplots()`. Argue from the graph that the petal length is either between 1 and 2 cm. or between 3 and 7 cm. but that no flowers in the dataset have a petal length between 2 and 3 cm. Do you think this could be useful to discriminate between the different types of flowers?\n",
    "> *Hint:* Check the documentation using `help(plt.subplots)`.\n",
    "\n",
    "> *Hint:* Use `plt.hist()` to plot a histogram.\n",
    "\n",
    "> *Hint:* Use indexing to extract each attribute. For example, `df.iloc[:, j-1]` extracts the $j$'th attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7869fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82cdd64dc86794e4fa3c4b66b470f806",
     "grade": false,
     "grade_id": "cell-57b0362ca1af32ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ba5f2",
   "metadata": {},
   "source": [
    "**Task 1.9:** Produce a boxplot of the four attributes in the Iris data. This boxplot shows the same information as the histogram in the previous exercise. Discuss the advantages and disadvantages of the two types of plots.\n",
    "> *Hint:* Use the function `plt.boxplot()` for creating the figure.\n",
    "\n",
    "- *Answer:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c43c9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80e55fb83006d2427376088d130f3ac9",
     "grade": false,
     "grade_id": "cell-9542b906997d3357",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed26152",
   "metadata": {},
   "source": [
    "**Task 1.10:** Create a figure using `plt.subplots()` that contains boxplots for each attribute for each class as in Figure 7.2 in the course book. Show on the graph that all the Iris-setosa in this dataset have a petal length between 1 and 2 cm. Do you think we would be able to distinguish between the Iris types from the measured sepal and petal length? Why/why not?\n",
    "\n",
    "> *Hint:* You can split the Pandas dataframe into subsets based on a specific attribute using `df.groupby()`.\n",
    "\n",
    "> *Hint:* Make sure to specify `sharey=True` when creating the subplot structure. This allows us to compare the values more easily.\n",
    "\n",
    "- *Answer:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319441ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "813e802903ad3eacf39fc3cd67574d9e",
     "grade": false,
     "grade_id": "cell-e0d358ae4d39975e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f5f25",
   "metadata": {},
   "source": [
    "**Task 1.11:** Create a matrix scatter plot using `plt.subplots()` of each combination of two attributes against each other as shown in in Figure 7.6 in the course book. Say you want to discriminate between the three types of flowers using only the length and width of either sepal or petal. Argue from the graph why it would be better to use petal length and width rather than sepal length and width.\n",
    "> *Hint:* To make a scatter plot, you can use the function `plt.scatter()` and fill the plots with two for-loops. Remember to keep track of the axes!\n",
    "\n",
    "> *Hint:* Another way to extract the data of a specific class, is to write `df.query('Type == \"Iris-setoa\"')`.\n",
    "\n",
    "> *Hint:* For the diagonal plots, we could also choose more informative plots than the scatter plot. Try filling it with the histograms.\n",
    "\n",
    "- *Answer:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c21cac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88bf3f1e3b992f5c7fb9a3b272e0f901",
     "grade": false,
     "grade_id": "cell-ae92598521001188",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda666a",
   "metadata": {},
   "source": [
    "**Task 1.12:** Produces a 3-dimensional scatter plot of three attributes as shown in Figure 7.7 in the course book. Try rotating the data. Can you find an angle where the three types of flower are separated in the plot? Discuss the pros and cons of visualizing data in 2 and 3 dimensions, respectively? How would you plot data that is inherently 4 dimensional or higher?\n",
    "> *Hint:*  You can add a 3D subplot element by `ax = fig.add_subplot(111, projection='3d')`. Read more about plotting in 3 dimensions [here](matplotlib.sourceforge.net/mpl_toolkits/mplot3d/tutorial.html).\n",
    "\n",
    "> *Hint:* In Jupyter notebooks, you can open the plots in a separate window (for interaction) by writing the cell magic `%matplotlib qt` in the top of the cell. Contrarily, what you've done so far is inline plotting with `%matplotlib inline`.\n",
    "\n",
    "- *Answer:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18df217",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1305d616f68b5b0a29659999cba9baba",
     "grade": false,
     "grade_id": "cell-803edee3a79fbfbc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "# This opens an interactive window for 3D plotting. \n",
    "# If you have problems use `%matplotlib inline` instead.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79aca1",
   "metadata": {},
   "source": [
    "**Task 1.13:** Apply standardization to the data matrix $\\boldsymbol{X}$ that we constructed in the first part of the exercise so that it has zero mean and unit standard deviation. Plot the standardized data matrix as an image. What does this plot indicate?\n",
    "> *Hint:* You can use the function `plt.imshow()` to plot an image. Check out the documentation of this function.\n",
    "\n",
    "> *Hint for interpreting the plot:* the data matrix is ordered according to the sorted class labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971de1c6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a69419521b866b77e1126279d8754079",
     "grade": false,
     "grade_id": "cell-ac5dd27ff568e848",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# go back to inline plotting\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0faad",
   "metadata": {},
   "source": [
    "A question that we need to ask ourselves is whether the dataset is suitable for solving our primary machine learning aim, which we here define as being classification. \n",
    "\n",
    "**Task 1.14:** Do you think that we would be able to fit a classification model on the Iris dataset? Argue based on the figures you generated. What if we change the primary machine learning aim to be the regression task mentioned earlier?\n",
    "\n",
    "> *Hint:* Consider which attributes we defined as *target* and *input* attributes, respectively, for the two modeling tasks.\n",
    "\n",
    "- *Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc571a8",
   "metadata": {},
   "source": [
    "You are welcome to try out other plotting methods for the data. Matplotlib online repository is a good source of inspiration: https://matplotlib.org/stable/gallery/index.html\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d3b92",
   "metadata": {},
   "source": [
    "## Part 2: Cleaning up data in Python\n",
    "\n",
    "While the Iris dataset is a real dataset, it is a very clean and easy to work with dataset. Usually, data is a bit messier, and we will consider a toy dataset that has some common issues. Often, the description of \"real-world\" data is stored along with the data in some form of a text file. Have a look at the folder `messy_data` in the data folder and read more about the toy dataset - notice that there is a `README.txt` file.\n",
    "\n",
    "**Task 2.1:** Inspect the data in `messy_data.data` and try to identify some issues (use e.g. simple text editor as before). What issues did you find?\n",
    "\n",
    "- *Answer:* \n",
    "\n",
    "**Task 2.2:** Load the messy dataset using `pd.read_csv()`.\n",
    "> *Hint:* Even though `messy_data.data` is not a `.csv`-file, we can load it by specifying the argument `sep=\\t` as the values are tab-separated.\n",
    "\n",
    "> *Hint:* What index is the header in the file? Specify this with the `header` argument.\n",
    "\n",
    "> *Hint:* To remove the header from the values of the dataframe, use `messy_data.drop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5c994",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25b62295b1dd95883b3c26d0249484f1",
     "grade": false,
     "grade_id": "cell-59eb934416fa3d89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the messy data\n",
    "N, M = messy_data.shape\n",
    "assert N == 29, \"There should be 29 samples in the messy dataset.\"\n",
    "assert M == 9, \"There should be 9 features in the messy dataset.\"\n",
    "\n",
    "# Display the messy dataframe\n",
    "messy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77d22b",
   "metadata": {},
   "source": [
    "At this point, youll see that some of the missing values from the data has already been represented as NaNs (in the displacement column). However, these were only the places where an empty element was in the file. We also see that the weight attribute uses ' as the thousand separator which is bad practice in Python. For the acceleration attribute we even see inconsistency in whether commas or dots are used as the decimal separator - in Python, we use dots.\n",
    "\n",
    "**Task 2.3:** Remove the question marks in displacement and replace them with not a number, i.e. `NaN` and solve the problems of the separator signs for the weight and acceleration attribute.\n",
    "> *Hint:* Use the method `.str.replace(\"?\", \"NaN\")` to modify the specific attribute on the string level for each data input. Use the same function for handling the separator issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601003c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3092fa7955775517700ba821b04c7e26",
     "grade": false,
     "grade_id": "cell-a8082107c0b5e77e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c4337",
   "metadata": {},
   "source": [
    "In the `README.txt` it is stated that \"zeroes in the attributes MPG and displacement can be considered missing values\". \n",
    "\n",
    "**Task 2.4:** Replace the zeros in these attributes, since a zero might be correct for some other variables.\n",
    "> *Hint:* You can use `.replace` like before, but do not need to do it on the string level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a97b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bcccd4dca2643fb791bbf8a87cb9aae",
     "grade": false,
     "grade_id": "cell-7e169a483455df42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac0ace",
   "metadata": {},
   "source": [
    "The `README.txt` does not supply a lot of information about what the levels of the \"origin\" attribute describe, so we either have to make an educated guess based on the values in the context, or preferably obtain the information from any papers that might be references in the `README`. From inspection of \"origin\" and \"car names\", you should see that (north) American cars are valued by 1, European cars are valued by 2 and Asian cars are valued by 3.\n",
    "\n",
    "**Task 2.5:** Convert the \"origin\" attribute to a categorical attribute labeled as described above. Next, apply one-out-of-$K$ encoding to the attribute (as we did with the Iris data) and remove the `carname` attribute.\n",
    "> *Hint:* Convert the attribute to a categorical Pandas attribute. The you can use the method `.rename_categories()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57712537",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0395d3a60fdd67cad4b80405489f2b02",
     "grade": false,
     "grade_id": "cell-9523185a34fb23fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7edb3",
   "metadata": {},
   "source": [
    "We later on find out that a value of 99 for the MPG is not value that is within reason for the MPG of the cars in this dataset, hence it is an outlier. The observations that has this value of MPG is therefore incorrect, and we should treat the value as missing. So far, the data has been of string type, however, it is easier to define filters if we convert the numerical attributes to being numerical.\n",
    "\n",
    "**Task 2.6:** Convert the numerical attributes into numerical-valued columns in the Pandas datafram. Add a line of code to remove the data point (rows) where MPG $= 99$.\n",
    "> *Hint:* You can index multiple columns of the dataframe using a list of column names, i.e. `messy_data[[name1, name2, ...]]`\n",
    "\n",
    "> *Hint:* Use `.astype(float)` to convert the string-valued attributes to numerical.\n",
    "\n",
    "> *Hint:* For filtering out the outlier, you can create a mask like `messy_data.mpg != x` and apply it to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7e144",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14102333a3587acb1bfdfeb0105156c0",
     "grade": false,
     "grade_id": "cell-a8b42b9ea5ef9c41",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "N, M = messy_data.shape\n",
    "assert N == 28, \"There should be 28 samples in the cleaned messy dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46f335",
   "metadata": {},
   "source": [
    "We still have the missing values. In the following we will go through how you could go about handling the missing values before making the $\\left(\\boldsymbol{X},\\boldsymbol{y}\\right)$-matrices as above. Various apporaches can be used, but it is important to keep it mind to never do any of them blindly. Keep a record of what you do, and consider/discuss how it might affect your modelling.\n",
    "\n",
    "The simplest way of handling missing values is to drop any records that display them, we do this by first determining where there are missing values.\n",
    "\n",
    "**Task 2.7:** Using Python, determine which observations (rows) contain missing values. Next, remove all observations that holds at least 1 missing value. Store the resulting dataframe in a variable called `clean_data`.\n",
    "> *Hint:* You can use `.isna().any()` to identify observations with NaN values. What axis should `.any()` be applied on?\n",
    "\n",
    "> *Hint:* The operator `~` negates the values of a boolean array, i.e. if `A = np.array([True, False])` then `~A = np.array([False, True])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4552da",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abc074629aa7967e408468d2701c6c3f",
     "grade": false,
     "grade_id": "cell-d8f17c022c122b75",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the cleaned data\n",
    "N, M = clean_data_v1.shape\n",
    "assert N == 15, \"There should be 24 samples in the cleaned messy dataset.\"\n",
    "assert M == 10, \"There should be 10 features in the cleaned messy dataset after one-out-of-K encoding.\"\n",
    "\n",
    "# Display the cleaned dataframe\n",
    "clean_data_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051329c8",
   "metadata": {},
   "source": [
    "Another approach to handling missing values is to check whether the majority of missing values comes from specific attributes. By visual inspection (either from plotting the dataframe using `plt.imshow()` or by checking the dataframe values), we see that the third column, i.e. the displacement attribute, is the major reason we have missing values.\n",
    "\n",
    "**Task 2.8:** Go back to the `messy_data` dataframe. Remove the displacement attribute (for now) and then remove the few observations (rows) containing missing values. Store the resulting dataframe in a variable `clean_data_v2`.\n",
    "> *Hint:* Another way to remove rows with missing values is using the Pandas method `.dropna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c70516",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f8e17160e741170eb07246086a4dded",
     "grade": false,
     "grade_id": "cell-a101d3deb3603f6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the cleaned data\n",
    "N, M = clean_data_v2.shape\n",
    "assert N == 26, \"There should be 26 samples in the cleaned messy dataset after removing displacement.\"\n",
    "assert M == 9, \"There should be 9 features in the cleaned messy dataset after removing displacement.\"\n",
    "\n",
    "# Display the cleaned dataframe\n",
    "clean_data_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea63b9",
   "metadata": {},
   "source": [
    "Lastly, one could impute the missing values - which means to \"guess them\", in some sense - while trying to minimize the impact of the guess. A simply way of imputing them is to replace the missing values with the median of the attribute. In our specific case, we would have to do this for the missing values for attributes MPG and displacement.\n",
    "\n",
    "**Task 2.9:** Go back to the `messy_data` dataframe. Replace missing values in the MPG and displacement columns with their respective median values. Store the resulting dataframe in a varible called `clean_data_v3`.\n",
    "> *Hint:* For computing the median of an attribute containing NaN-values, use `np.nanmedian` or the Pandas method `.median()`. We will take a closer look on summary statistics next week.\n",
    "\n",
    "> *Hint:* Checkout the Pandas method `.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf55188",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05ef75166d2329fe7ad16e89cd6c3875",
     "grade": false,
     "grade_id": "cell-1db1374a9aeca777",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "N, M = clean_data_v3.shape\n",
    "assert N == 28, \"There should be 28 samples in the cleaned messy dataset after filling missing values.\"\n",
    "assert M == 10, \"There should be 10 features in the cleaned messy dataset after filling missing values.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28948676",
   "metadata": {},
   "source": [
    "**Task 2.10:** Which of the methods do you prefer? Which of the cleaned data versions contains most information? Why is this useful in a machine learning context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c866c",
   "metadata": {},
   "source": [
    "Perfect! Now our data is cleaned up and we would actually be able to use it for solving a regression or classification task! Let's try to construct the $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$ matrices. One idea could be to try to predict the weight of the cars based on the remaining attributes. This means that we will have to solve a regression problem.\n",
    "\n",
    "**Task 2.11:** Split the `clean_data_v3` into the $\\left(\\boldsymbol{X}, \\boldsymbol{y}\\right)$-format such that the target attribute is \"weight\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f2b9f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "763403e2b1390900812ca593bbc9f86a",
     "grade": false,
     "grade_id": "cell-ae6cb7275e0b86f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385016e",
   "metadata": {},
   "source": [
    "Given the data split, we will sometimes need to consider common feature transformations such as standardization and binarization/thresholding. Standardization can be useful for being able to compare data attributes measured on very different scales and works by subtracting the mean and dividing by the standard deviation for each attribute, respectively, and sometimes it is also important to standardize the target attribute itself! On the other hand, binarization and thresholding can be used to e.g. construct a discrete classification target attribute from a continuous-valued attribute.\n",
    "\n",
    "**Task 2.12:** Create a standardized version of the data matrix called `X_standardized`. Do the same for the target attribute and store it in `y_standardized`. Lastly, discretize the weight target attribute into 3 categories being \"low\", \"medium\" and \"high\".\n",
    "> *Hint:* Use Pandas methods `.mean()` and `.std()` when standardizing $\\boldsymbol{X}$ and $\\boldsymbol{y}$. What axis should you compute the values over?\n",
    "\n",
    "> *Hint:* To discretize a continuous attribute with Pandas, use `pd.cut()`. Check the documentation with `help(pd.cut)` for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8d44a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15157848a715792a888ac8b6e5c15a8e",
     "grade": false,
     "grade_id": "cell-16452ebd5a0bd998",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the standardized features and target variable\n",
    "assert (X_standardized.mean(axis=0) < 1e-5).all(), \"The mean of the standardized features should be close to 0.\"\n",
    "assert (abs(X_standardized.std(axis=0) - 1) < 1e-5).all(), \"The standard deviation of the standardized features should be close to 1.\"\n",
    "assert (y_standardized.mean() < 1e-5), \"The mean of the standardized target variable should be close to 0.\"\n",
    "assert abs(y_standardized.std() - 1) < 1e-5, \"The standard deviation of the standardized target variable should be close to 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91d525",
   "metadata": {},
   "source": [
    "**Optional:** Below we show an example of how to use the dataset to fit a supervised learning method. In this scenario we consider a linear regression model. In the coming weeks, we will get to know many more supervised learning techniques. We evaluate the predictions using the root-mean-square error - how can we interpret this value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Example of fitting a supervised learning model, e.g. a linear regression model, using sklearn\n",
    "model = LinearRegression()      # define the model\n",
    "model.fit(X, y)                 # fit the model to the data\n",
    "y_hat = model.predict(X)        # predict the target variable using the model on all data\n",
    "\n",
    "# Compute the RMSE (Root Mean Squared Error)\n",
    "# this is a common metric for regression tasks as it measures the average magnitude \n",
    "# of the errors between predicted and actual values.\n",
    "RMSE = np.sqrt(np.mean(np.power(y_hat - y, 2)))\n",
    "print(f\"RMSE: {RMSE:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981cd2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Text-representation in Python (Optional)\n",
    "\n",
    "In the previous tasks, we learned how to load and manipulate tabular data as well as some techniques for visualizing the attributes. However, not all data follows the tabular data structure and we will presently consider one such example.\n",
    "\n",
    "An important area of research in machine learning and data mining is the analysis of text documents. Here, important tasks are to be able to search documents as well as group related documents together (clustering). In order to accomplish these tasks the text documents is converted into a format suitable for data modeling. We will use the **bag of words** representation. Here, text documents are stored in a matrix $\\boldsymbol{X}$ where $x_{ij}$ indicate how many times word $j$ occurred in document $i$.\n",
    "\n",
    "Suppose that we have 5 text documents, each containing just a single sentence:\n",
    "\n",
    "> Document 1: The Google matrix $P$ is a model of the internet.\n",
    ">\n",
    "> Document 2: $P_{ij}$ is nonzero if there is a link from webpage $i$ to $j$.\n",
    ">\n",
    "> Document 3: The Google matrix is used to rank all Web pages.\n",
    ">\n",
    "> Document 4: The ranking is done by solving a matrix eigenvalue problem. \n",
    ">\n",
    "> Document 5: England dropped out of the top 10 in the FIFA ranking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da52de9",
   "metadata": {},
   "source": [
    "**Taske 3.1:** Propose a suitable **bag of words** representation for these documents (use pen and paper). You should choose approximately 10 key words in total defining the columns in the document-term matrix and the words are to be chosen such that each document at least contains 2 of your key words, i.e. the document-term matrix should have approximately 10 columns and each row of the matrix must at least contain 2 non-zero entries.\n",
    "- *Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f2cbe",
   "metadata": {},
   "source": [
    "In practice, we can carry out this procedure automatically using the scikit-learn library, or `sklearn`. We will use a function from the feature extraction-module, called `CountVectorizer` to generate a document-term matrix and to convert it into the course format, i.e. $\\boldsymbol{X}$. Note, that we will use the words \"term\" and \"token\" interchangeably.\n",
    "\n",
    "**Task 3.2:** Inspect the `textDocs.txt`-file provided in the associated data folder.\n",
    "\n",
    "As you might have seen, the data is no longer following a fixed structure as in the CSV-files that we previously worked with. Instead of using Pandas for loading the data we will directly read the lines of the `txt`-file. Read and understand the code for loading the documents in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the txt-file and read its content\n",
    "with open('data/BoW/textDocs.txt', 'r') as f:\n",
    "    raw_file = f.read()\n",
    "\n",
    "# The raw file is a single string with all content of the file\n",
    "# We need to split it into individual documents/sentences using \\n as the delimiter\n",
    "corpus = raw_file.split('\\n')\n",
    "\n",
    "# Next, we remove the empty lines from the corpus\n",
    "corpus = list(filter(None, corpus))\n",
    "\n",
    "# Display the content of the corpus\n",
    "print(\"Corpus (5 documents/sentences):\")\n",
    "print(np.asmatrix(corpus))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f5941",
   "metadata": {},
   "source": [
    "**Task 3.3:** Construct the document-term matrix as a `np.array` using `CountVectorizer` and compare the generated document-term matrix to the one you generated yourself.\n",
    "> *Hint:* Make sure that you have the `sklearn`-package installed.\n",
    "\n",
    "> *Hint:* Read more about the `CountVectorizer` using `help(CountVectorizer)` after it has been imported from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6c678",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75640c08253ad03c21fb20f56e36c4bf",
     "grade": false,
     "grade_id": "cell-8e904edfce3b0ed9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We define a CountVectorizer to convert the corpus into a document-term matrix.\n",
    "# The token pattern is a regular expression (marked by the r), which ensures\n",
    "# that the vectorizer ignores digit/non-word tokens - in this case, it ensures\n",
    "# the 10 in the last document is not recognized as a token. It's not important\n",
    "# that you should understand it the regexp.\n",
    "vectorizer = CountVectorizer(token_pattern=r\"\\b[^\\d\\W]+\\b\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the document-term matrix\n",
    "N, M = X.shape\n",
    "assert N == 5, \"There should be 5 documents in the corpus.\"\n",
    "assert M == 36, \"There should be 36 terms in the document-term matrix.\"\n",
    "\n",
    "print(\"Number of documents (data points, N):\\t %i\" % N)\n",
    "print(\"Number of terms (attributes, M):\\t %i\" % M)\n",
    "print(\"\\nTerms in the document-term matrix:\")\n",
    "print(terms)\n",
    "print(\"\\nDocument-term matrix:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd367935",
   "metadata": {},
   "source": [
    "Stop words are words that one can find in virtually any document. Therefore, the occurrence of such a word in a document does not distinguish the document from other documents. The following is the beginning of one particular stop word list:\n",
    "> a, a's, able, about, above, according, accordingly, across,   actually, after, afterwards, again, against, ain't, all, allow,  allov, ahnost, alone, along, already, also, although, always, am,  among, amongst, an, and, another, any, anybody, anyhow, anyone,  anything, anyway, anyways, anywhere, apart, appear, appreciate,  appropriate, are, around, as, aside,ask, ....\n",
    "\n",
    "When forming the document-term it is common to remove these specified stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afadea5",
   "metadata": {},
   "source": [
    "The generated document-term matrix contains words that carry little information such as the word \"the\". We will remove these words as they can be interpreted as \"noise\" carrying no information about the content of the documents. \n",
    "\n",
    "**Task 3.4:** Load the stop words from the `stopWords.txt`-file in the associated data folder. Compute a new document-term matrix with stop words removed - how does it compare to your original\n",
    "matrix?\n",
    "\n",
    "> *Hint:* You can load the stop words similarly to how we loaded the documents. \n",
    "\n",
    "> *Hint:* Once the stop words are loaded, they can be parsed to the `CountVectorizer` using the keyword `stop_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7c92f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34290212ceb8dd3d93e81fae52a4afd2",
     "grade": false,
     "grade_id": "cell-0caae2e5ba50c30c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the document-term matrix\n",
    "N, M = X.shape\n",
    "assert N == 5, \"There should be 5 documents in the corpus.\"\n",
    "assert M == 19, \"There should be 19 terms in the document-term matrix.\"\n",
    "print(\"Number of documents (data points, N):\\t %i\" % N)\n",
    "print(\"Number of terms (attributes, M):\\t %i\" % M)\n",
    "\n",
    "# Show the document-term matrix as a Pandas dataframe for better overview\n",
    "pd.DataFrame(X, columns=terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a4241",
   "metadata": {},
   "source": [
    "Stemming denotes the process for reducing inflected (or sometimes derived) words to their stem, base or root form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Clearly, from the point of view of information retrieval, no information is lost in the following stemming reduction:\n",
    "$$\n",
    "    \\begin{equation*}\n",
    "    \\left.\\begin{array}{l}\n",
    "        \\text{computable}\\\\\n",
    "        \\text{computing}\\\\\n",
    "        \\text{computed}\\\\\n",
    "        \\text{computational}\\\\\n",
    "        \\text{computation}\\\\\n",
    "    \\end{array}\\right\\}\\rightarrow \\text{comput}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Document 3, 4 and 5 have the word \"rank\" in common. However in document 4 and 5 this word is stored as a the separate word entry \"ranking\" in the document-term matrix whereas in document 3 it is stored as the word entry \"rank\". As such, the document-term matrix does not indicate that document 3, 4 and 5 share the word \"rank\". By the use of stemming we can obtain a matrix that indicate that the word \"rank\" appears in all 3 documents. In the following cell, we will show you how to apply the the `PorterStemmer` from the `nltk` package and create a new document-term matrix.\n",
    "> Make sure you have the `nltk`-package installed.\n",
    "\n",
    "**Task 3.5:** Inspect the document-term matrix after stemming. How does it compare to your original matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a widely used stemmer based:\n",
    "# Porter, M. âAn algorithm for suffix stripping.â Program 14.3 (1980): 130-137.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Make an object based on the PorterStemmer class\n",
    "stemmer = PorterStemmer()\n",
    "# Construct an analyzer for generating the document-term matrix\n",
    "analyzer = CountVectorizer(\n",
    "    token_pattern=r\"\\b[^\\d\\W]+\\b\", \n",
    "    stop_words=stopwords\n",
    ").build_analyzer()\n",
    "\n",
    "# Using this we make a function that can stem words:\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "# ... and finally, we create a new vectorizer just like we've done before:\n",
    "vectorizer_with_stemming = CountVectorizer(analyzer=stemmed_words)\n",
    "\n",
    "# Fit the vectorizer to the corpus\n",
    "vectorizer_with_stemming.fit(corpus)\n",
    "# Extract the terms/tokens from the vectorizer\n",
    "terms = vectorizer_with_stemming.get_feature_names_out()\n",
    "# Transform the corpus into the document-term matrix\n",
    "X = vectorizer_with_stemming.transform(corpus).toarray()\n",
    "\n",
    "# Check the shape of the document-term matrix\n",
    "N, M = X.shape\n",
    "assert N == 5, \"There should be 5 documents in the corpus.\"\n",
    "assert M == 18, \"There should be 18 terms in the document-term matrix.\"\n",
    "print(\"Number of documents (data points, N):\\t %i\" % N)\n",
    "print(\"Number of terms (attributes, M):\\t %i\" % M)\n",
    "\n",
    "# Show the document-term matrix as a Pandas dataframe for better overview\n",
    "pd.DataFrame(X, columns=terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdb318",
   "metadata": {},
   "source": [
    "Based on our document-term representation we can now make simple searches (queries) in our documents based on some form of similarity measure between our query vector and document-term representation. Lets say we want to find all documents that are relevant to the query \"**solving** for the **rank** of a **matrix**.'' This is represented by a query vector, $\\boldsymbol{q}$, constructed in a way analogous to the document-term matrix, $\\boldsymbol{X}$, hence\n",
    "$$\n",
    "    \\boldsymbol{q} =Â \\left[\\begin{array}{ccccccccccccccccc}\n",
    "      0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0\n",
    "    \\end{array}\\right]^\\top\n",
    "$$\n",
    "\n",
    "We will use the **cosine distance** as a measure of similarity between the $i$'th document $\\boldsymbol{x}_i$ and the query vector $\\boldsymbol{q}$, i.e. \n",
    "$$\n",
    "  \\mathrm{cos}(\\boldsymbol{q},\\boldsymbol{x}_i)=\\frac{\\boldsymbol{q}}{\\|\\boldsymbol{q}\\|}\\cdot\\frac{\\boldsymbol{x}_i}{\\|\\boldsymbol{x}_i\\|} =\\frac{\\boldsymbol{q}^\\top\\boldsymbol{x}_i}{\\|\\boldsymbol{q}\\|\\|\\boldsymbol{x}_i\\|}\n",
    "$$\n",
    "We will learn much more about measures of similarity next week. \n",
    "\n",
    "**Task 3.6:** Compute the cosine similarity between each document and the query using a) pen and paper (i.e. compute the inner products between the relevant vectors) and b) using `numpy`. Explain what documents, according to our similarity measure, are most related to the query and verify that Document 4 is the most similar one.\n",
    "> *Hint:* You can extract a document (row of the `X` matrix) using the command `x=X[i, :]` `i` is the index of the document.\n",
    "\n",
    "> *Hint:* Numpy matrices and arrays can be transposed using notation `x.T` or `x.transpose()`.\n",
    "\n",
    "> *Hint:* Dot products between two row vectors can be computed as `np.dot(q, x.T)` (or simply `q @ x.T`).\n",
    "\n",
    "> *Hint:* The norm of a vector can be computed using the function `np.linalg.norm()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b147f34",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e459a6792fcdb081201e39eea83af4e1",
     "grade": false,
     "grade_id": "cell-a44db501a9b53f3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the query vector\n",
    "q = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
    "# Print terms in the query vector\n",
    "print(\"Terms in the query vector: \", terms[q == 1])\n",
    "\n",
    "# notice, that you could get the query vector using the vectorizer, too:\n",
    "# q = vectorizer_with_stemming.transform(['matrix rank solv'])\n",
    "# q = np.asarray(q.toarray())\n",
    "# print(q)\n",
    "# or use any other string:\n",
    "# q = vectorizer_with_stemming.transform(['Can I Google how to fix my problem?'])\n",
    "# q = np.asarray(q.toarray())\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Display the result\n",
    "print(\"Query vector:\\n {0}\\n\".format(q))\n",
    "print(\"Similarity results:\\n {0}\".format(cosine_similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a53fb",
   "metadata": {},
   "source": [
    "**Optional:** If you find text processing exciting, read more about Natural Language Processing toolkit. Here is a good place to start: http://www.nltk.org/book/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02450-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
