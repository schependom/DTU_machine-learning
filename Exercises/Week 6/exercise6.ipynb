{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a188550",
   "metadata": {},
   "source": [
    "**02452** *Machine Learning*, Technical University of Denmark\n",
    "\n",
    "- This Jupyter notebook contains exercises where you fill in missing code related to the lecture topic. *First*, try solving each task yourself. *Then* use the provided solution (an HTML file you can open in any web browser) as inspiration if needed. If you get stuck, ask a TA for help.\n",
    "\n",
    "- Some tasks may be difficult or time-consuming - using the solution file or TA support is expected and perfectly fine, as long as you stay active and reflect on the solution.\n",
    "\n",
    "- You are not expected to finish everything during the session. Prepare by looking at the exercises *before* the class, consult the TAs *during* class, and complete the remaining parts *at home*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba172393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd62d4",
   "metadata": {},
   "source": [
    "# Week 6: Overfitting and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d2ec3a",
   "metadata": {},
   "source": [
    "**Content:** \n",
    "- Part 1: Understanding over-fitting\n",
    "- Part 2: Approximating Generalization Error through Cross-Validation. \n",
    "- Part 3: Cross-Validation for Model Evaluation\n",
    "- Part 4: Feature Selection through nested cross-validation.\n",
    "\n",
    "**Objectives:**\n",
    "- Understand under- and overfitting. \n",
    "- Understand the difference between training, test and generalization error.\n",
    "- Explain how cross-validation can be used for (i) performance evaluation and (ii) model selection.\n",
    "- Be able to do feature-selection, and understand how it relates to over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "# Random seed\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Function for plotting polynomial data\n",
    "def plot_polynomial_data(X_train, y_train, X_test, y_test, func, model = None, plot_test_points=True, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot training points\n",
    "    ax.plot(X_train, y_train, \"o\", color=\"tab:red\", label=\"Training data\", mec='k')\n",
    "\n",
    "    # Plot test points and true function.\n",
    "    if plot_test_points: # only plot test points if specified\n",
    "        ax.plot(X_test, y_test, \"x\", color=\"tab:blue\", label=\"Test data\", alpha=0.5)\n",
    "    \n",
    "    # True function\n",
    "    ax.plot(X_test, func(X_test), \"--\", color=\"gray\", label=\"True function\")\n",
    "    \n",
    "    # Plot model predictions if given\n",
    "    if model is not None: # only plot predictions if specified\n",
    "        x_grid = np.linspace(-1, 1, 1000).reshape(-1,1)\n",
    "        ax.plot(x_grid, model.predict(x_grid), color=\"tab:green\", label=f\"Fitted model\")\n",
    "\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_ylim(-6, 6)\n",
    "\n",
    "def plot_fold_errors(fold_errors):\n",
    "    K = fold_errors.shape[0]\n",
    "\n",
    "    # Scale figure width with number of folds\n",
    "    base_width = 6   # minimum width\n",
    "    width_per_fold = 0.25  # additional width per fold\n",
    "    max_width = 20   # cap to prevent it from getting absurdly wide\n",
    "    fig_width = min(base_width + K * width_per_fold, max_width)\n",
    "    fig_height = 6\n",
    "    f = plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(K)\n",
    "    plt.bar(index, fold_errors[:, 0], bar_width, color='tab:blue', label='Train error')\n",
    "    plt.bar(index + bar_width, fold_errors[:, 1], bar_width, color='tab:red', label='Test error')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.ylim(0, max(fold_errors.flatten()) + 0.2)\n",
    "    plt.xlim(-0.5, K - 0.5 + bar_width)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x')\n",
    "\n",
    "    if K > 20:\n",
    "        step = max(1, K // 20)  # show ~20 labels max\n",
    "        plt.xticks(index[::step] + bar_width / 2, [f'{i+1}' for i in range(0, K, step)])\n",
    "\n",
    "    else:\n",
    "        plt.xticks(index + bar_width / 2, [f'{i+1}' for i in range(K)])\n",
    "\n",
    "\n",
    "def bmplot(yt, xt, X):\n",
    "    \"\"\"\n",
    "    Function plots matrix X as image with white background, black grid lines, and dark blue for selected cells.\n",
    "    \"\"\"\n",
    "    plt.imshow(X, interpolation=\"none\", cmap=\"Blues\", aspect=\"equal\", vmin=0, vmax=1)\n",
    "    plt.xticks(range(0, len(xt)), xt)\n",
    "    plt.yticks(range(0, len(yt)), yt)\n",
    "    plt.gca().set_facecolor(\"white\")\n",
    "    for i in range(0, len(yt)):\n",
    "        plt.axhline(i - 0.5, color=\"black\")\n",
    "    for i in range(0, len(xt)):\n",
    "        plt.axvline(i - 0.5, color=\"black\")\n",
    "    plt.grid(False)\n",
    "\n",
    "\n",
    "def plot_seq_feature_results(fold_results, feature_names):\n",
    "    num_folds = len(fold_results)\n",
    "\n",
    "    for i, res in enumerate(fold_results):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw={\"width_ratios\": [2, 1]})\n",
    "        \n",
    "        # Left: MSE progression\n",
    "        axes[0].plot(range(1, len(res[\"mse\"])+1), res[\"mse\"], marker=\"o\")\n",
    "        axes[0].set_title(f\"Fold {i+1}: MSE vs #features\")\n",
    "        axes[0].set_xlabel(\"Iteration\")\n",
    "        axes[0].set_ylabel(\"MSE\")\n",
    "\n",
    "        # Right: Heatmap of feature selection order using bmplot\n",
    "        grid = np.zeros((len(feature_names), len(res[\"features\"])))\n",
    "        for j, feat in enumerate(res[\"features\"]):\n",
    "            grid[feature_names.index(feat), j:] = 1\n",
    "\n",
    "        plt.sca(axes[1])\n",
    "        bmplot(feature_names, list(range(1, len(res[\"features\"])+1)), grid)\n",
    "        axes[1].set_title(f\"Fold {i+1}: feature selection path\")\n",
    "        axes[1].set_xlabel(\"Iteration\")\n",
    "        axes[1].set_ylabel(\"Features\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Final summary heatmap\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw={\"width_ratios\": [2, 1]})\n",
    "\n",
    "    # Left: Outer test MSEs\n",
    "    outer_mses = [res[\"outer_test_mse\"] for res in fold_results]\n",
    "    axes[0].bar(range(1, num_folds + 1), outer_mses, color='tab:blue')\n",
    "    axes[0].set_title(\"Outer test MSEs per fold\")\n",
    "    axes[0].set_xlabel(\"Fold\")\n",
    "    axes[0].set_ylabel(\"Outer test MSE\")\n",
    "    axes[0].grid(True, axis='y')\n",
    "    \n",
    "    # Right: Feature usage summary\n",
    "    summary = np.zeros((len(feature_names), num_folds))\n",
    "    for i, res in enumerate(fold_results):\n",
    "        for feat in res[\"features\"]:\n",
    "            summary[feature_names.index(feat), i] = 1\n",
    "\n",
    "    bmplot(feature_names, range(num_folds), summary)\n",
    "    axes[1].set_title(\"Feature usage across folds\")\n",
    "    axes[1].set_xlabel(\"Outer folds\")\n",
    "    axes[1].set_ylabel(\"Features\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7a7c0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "There are many ways to fit a model to data and achieve low training error, but this alone does not guarantee good performance on new, unseen data. A model that is too simple to capture the underlying structure, while one that is too complex may memorize noise rather than learn the underlying signal. In both cases, the result is poor generalization.\n",
    "\n",
    "Our goal is not just to explain the training data, but to build models that generalize - performing well on data they have never encountered.\n",
    "\n",
    "In this notebook, we will explore key concepts in machine learning:\n",
    "<br>Overfitting, cross-validation and feature selection.\n",
    "\n",
    "The central theme is the trade-off between model complexity and generalization ability. Too simple, and the model underfits; too complex, and it overfits. <br> We will look at practical ways to approximate generalization error, optimize modeling choices, and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0dcc6",
   "metadata": {},
   "source": [
    "## Part 1: Understanding over-fitting\n",
    "\n",
    "In this exercise, we will be demonstrating under- and overfitting on a simple 3rd degree polynomial, $y = 10x^3 - 5x$, plotted in the following cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "# Define data-generating function\n",
    "func = lambda x: 10 * x**3 - 5 * x\n",
    "# Generate training data from polynomial\n",
    "N_train = 15\n",
    "X_train = np.linspace(-1, 1, N_train).reshape(-1,1) # Training inputs\n",
    "y_train = func(X_train) + np.random.randn(N_train).reshape(-1,1) # Training outputs with noise\n",
    "# Generate test data from polynomial\n",
    "N_test = 100\n",
    "X_test = np.linspace(-1,1, N_test).reshape(-1,1) # Test inputs\n",
    "y_test = func(X_test) + np.random.randn(N_test).reshape(-1,1) # Test outputs with noise\n",
    "\n",
    "# Combine train and test data for later use. \n",
    "X = np.vstack((X_train, X_test))\n",
    "y = np.vstack((y_train, y_test))\n",
    "\n",
    "# Plot the data using the function\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_polynomial_data(X_train, y_train, X_test, y_test=y_test, func=func, ax=ax)\n",
    "ax.set_title(\"Polynomial data with noise\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb6222",
   "metadata": {},
   "source": [
    "Last week we saw that the parameters of a linear regression model can be found by minimizing the least squares error. In matrix form, this gives a closed-form solution for the weights.\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{w}^\\ast = \\left(\\tilde{\\boldsymbol{X}}^\\top \\tilde{\\boldsymbol{X}}\\right)^{-1} \\tilde{\\boldsymbol{X}}^\\top \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "In practice, instead of computing this directly, we can rely on `sklearn.linear_model.LinearRegression()` to estimate the model.\n",
    "\n",
    "If we want to extend linear regression to handle polynomials (for example, fitting curves instead of just straight lines), we can do this by first expanding our input features into polynomial terms - we also did this in closed-form last week. In scikit-learn, this expansion is handled by `sklearn.preprocessing.PolynomialFeatures(degree)`.\n",
    "\n",
    "To keep everything organized and automated, scikit-learn provides pipelines. A pipeline lets us chain together preprocessing steps (like adding polynomial features) with a final estimator (like linear regression). By specifying the degree in PolynomialFeatures, the pipeline will automatically transform the input data and then fit the regression model on the transformed version - the pipeline can be created with `sklearn.pipeline.make_pipeline()`.\n",
    "\n",
    "**Task 1.1:** Initialize the model-pipeline, choose a polynomial degree, and fit it on the training points. Can you make the model fit the true function well?\n",
    "\n",
    "> *Hint:* Remember to reshape `X_train`.\n",
    "\n",
    "> *Hint:* The plotting function takes the fitted model as input, and plots the predictions on a predefined grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b689a9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40b45125adb6e11d30425f44783bebd7",
     "grade": true,
     "grade_id": "cell-c3e8221a59180e43",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1.1) Fit polynomial model\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the data using the function, now with the fitted model\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_polynomial_data(X_train, y_train, X_test, y_test=y_test, func=func, model=model, ax=ax)\n",
    "ax.set_title(\"Polynomial data with noise\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d073634",
   "metadata": {},
   "source": [
    "**Task 1.2:** Experiment with different polynomial degrees. Can you make the model so simple that it simply fits a slope? \n",
    "> *Hint:* Make the model more simple by decreasing the degree.\n",
    "\n",
    "> *Hint:* This is what's known as underfitting. \n",
    "\n",
    "**Task 1.3:** Can you make the model so complex that it fits the training data exactly (matching every point)? \n",
    "> *Hint:* Make the model more complex by increasing the degree.\n",
    "\n",
    "> *Hint:* This is what's known as overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fb91f",
   "metadata": {},
   "source": [
    "We’ll now examine how the training and test errors change as we vary the polynomial degree, moving from underfitting to overfitting.\n",
    "\n",
    "**Task 1.4:** Use the model to predict on the train and test data, calculate the RMSE and save them as `RMSE_train` and `RMSE_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d9ecd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4fd0003332e71316a250f3e3a5b6f58",
     "grade": true,
     "grade_id": "cell-9e603441174203ad",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Setting up figure\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "fig.suptitle(\"Polynomial regression: under/overfitting\", fontsize=14)\n",
    "# Top: Three columns for three degrees\n",
    "gs = gridspec.GridSpec(2, 3, height_ratios=[2, 1])\n",
    "axes_top = [fig.add_subplot(gs[0, i]) for i in range(3)]\n",
    "# Bottom: Combine three column\n",
    "ax_bottom = fig.add_subplot(gs[1, :])  # span all 3 columns\n",
    "\n",
    "# Degrees and titles for the three columns\n",
    "degrees = [1, 3, 15]\n",
    "titles = [f\"Underfit (K={degrees[0]})\",\n",
    "          f\"Good fit (K={degrees[1]})\",\n",
    "           f\"Overfit (K={degrees[2]})\"]\n",
    "\n",
    "# Model fitting / prediction and plotting\n",
    "for ax, degree, title in zip(axes_top, degrees, titles):\n",
    "    \n",
    "    # Create and fit the model.\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "    # Plot the data and the model\n",
    "    plot_polynomial_data(X_train, y_train, X_test, y_test, func, model=model, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.grid(True)\n",
    "    ax.set_ylim(-6, 5)\n",
    "\n",
    "# Show legend and y-axis on first subplot\n",
    "axes_top[0].set_ylabel(\"y\")\n",
    "axes_top[0].yaxis.set_visible(True)\n",
    "axes_top[0].legend(loc=\"best\")\n",
    "\n",
    "# Fit and predict for K = 1..15\n",
    "max_degree = max(degrees)\n",
    "train_errors, test_errors = [], []\n",
    "\n",
    "for degree in range(1, max_degree + 1):\n",
    "\n",
    "    # Create and fit the model.\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "    # 1.4) Predict on training and test data, calculate the root mean square error and save them as RMSE_train and RMSE_test.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Store the errors\n",
    "    train_errors.append(RMSE_train)\n",
    "    test_errors.append(RMSE_test)\n",
    "\n",
    "ax_bottom.plot(range(1, max_degree + 1), train_errors, \"-o\", color=\"tab:red\", label=\"Train error\")\n",
    "ax_bottom.plot(range(1, max_degree + 1), test_errors, \"-o\", color=\"tab:blue\", label=\"Test error\")\n",
    "ax_bottom.set_xlabel(\"Polynomial degree (K)\")\n",
    "ax_bottom.set_ylabel(\"RMSE\")\n",
    "ax_bottom.set_title(\"Training vs. Test Error\")\n",
    "ax_bottom.legend()\n",
    "ax_bottom.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebe477",
   "metadata": {},
   "source": [
    "This should serve as an example of **over-** and **underfitting**. A model that’s too simple can’t capture the training data well, while a model that’s too complex ends up relying too heavily on the training data and fails on new, unseen test data.\n",
    "<br> In this case, you can see the model fitting the training points perfectly, but that “perfection” actually hurts its ability to generalize.\n",
    "\n",
    "The bottom plot shows this trade-off: the training error keeps going down as the model gets more complex, but at some point the test error starts to rise sharply. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed28b17",
   "metadata": {},
   "source": [
    "So how do we stop a model from overfitting? One way is to simplify the model by reducing the number of weights, as we have just seen above. <br> Another is to give it more training data, such that the model has a better chance to learn the real patterns, rather than the noise. In the coming weeks, we’ll also see how **regularization** can help address overfitting.\n",
    "\n",
    "**Task 1.5:** Increase the number of training points in the above examples. What happens to the overfitted model when $N_{train}=50, 100, 1000$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9205c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 2: Approximating Generalization Error through Cross-Validation\n",
    "\n",
    "In the above example, you saw how the **test error** is a much better representation of the true performance of the model than the **training error**. As you might have noticed in the code-block, we used the *holdout*-method, where the data is split into a single *training-* and *testset*. \n",
    "<br> In cases of small test-sets, this might lead to a crude approximation of the actual generalization error - as the data might be randomly be split into sets that don't represent the actual distribution well. \n",
    "We will therefore look at more comprehensive methods of estimating the **generalization error**, through **cross-validation**.\n",
    "\n",
    "As you might recall from the lecture, the generalization error is the test error evaluated over an *_infinitely_* large test set. <br> \n",
    "In other words, it's the \"true performance\" of a model, if we were to test it on an infinite amount of new, unseen data.\n",
    "\n",
    "Intuitively, this is simply not feasible in the real world, where data is often limited, and thus our only option is to **estimate** the generalization error instead. \n",
    "\n",
    "Cross-validation is a method we use to estimate the generalization error, and it is particularly useful when we don't have a lot of data. We will look at three different methods:\n",
    "1) **Holdout**: Partitions dataset in two (training, test), we approximate the generalization error based on the resulting test set.\n",
    "\n",
    "2) **K-fold**: Partitions dataset $K$ times, such that all observations get to act as test-points in a single fold. We train the model $K$ times and approximate the generalization error by averaging over the $K$ test results.\n",
    "\n",
    "3) **Leave-one-out**: Partitions dataset into as many folds as there are observations, such that each observation acts as a test point once. We train the model $N$ times (where $N$ is the number of samples) and approximate the generalization error by averaging over all $N$ test results.\n",
    "\n",
    "Each of the three methods has its own advantages and drawbacks. In general, increasing the number of folds gives us a more reliable estimate of the generalization error, but it also requires training more models and therefore consumes more computational resources. With the holdout method, there is always the risk of an “unlucky” split, where the training and test sets do not represent the overall data distribution well, leading to an unreliable performance estimate. <br>\n",
    "As a rule of thumb: the more data we have, the fewer folds we need.\n",
    "\n",
    "In this exercise, we will compare three approaches to splitting the data: **holdout**, **K-fold**, and **leave-one-out**. <br> For each method, we will once again fit a 3rd degree polynomial to the above data - and compare the results we find in each fold, as well as across methods. \n",
    "\n",
    "**Task 2.1:** Split $\\boldsymbol{X}$ and $\\boldsymbol{y}$ into a training and test split with the holdout method, using a 80/20 split.  \n",
    "> *Hint:* Use the function `train_test_split()`.\n",
    "\n",
    "> *Hint:* Remember to use `random_state = seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a968e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e00fbdbfa0dfd144ce2bc92c28e472d",
     "grade": true,
     "grade_id": "cell-50a43379f2821f4f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary to store errors for each method\n",
    "errors = {}\n",
    "degree = 3 # Use degree 3 for the rest of the exercise\n",
    "\n",
    "# 2.1) Holdout Method (20% test size) - split X and y into X_train, X_test, y_train, y_test\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Initialize model\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# Fit model\n",
    "model.fit(X_train.reshape(-1, 1), y_train)\n",
    "# Predict\n",
    "y_train_pred = model.predict(X_train.reshape(-1, 1))\n",
    "y_test_pred  = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "# Compute errors\n",
    "train_error = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_error  = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Bar plot \"holdout\" train and test error\n",
    "plot_fold_errors(np.array([(train_error, test_error)]))\n",
    "plt.title('Holdout method: train vs test error')\n",
    "plt.show()\n",
    "\n",
    "# Save mean errors for later comparison\n",
    "errors['Holdout'] = (train_error, test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce15082",
   "metadata": {},
   "source": [
    "**Task 2.2:** Initialize the K-fold method with $K=5$, save it as `CV_kfold`.\n",
    "> *Hint:* Use the function `KFold()`.\n",
    "\n",
    "> *Hint:* Remember to set `shuffle=True` and use `random_state=seed` - why is that important with K-fold?\n",
    "\n",
    "**Task 2.3:** Split $\\boldsymbol{X}$ and $\\boldsymbol{y}$ into a training and test split within each fold.\n",
    "\n",
    "> *Hint:* Index `X` and `y` by the indexes provided in the for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d7355",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf962a418bc5b92c260edc28e9f8f767",
     "grade": true,
     "grade_id": "cell-3c7bb6c49664b191",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "degree = 3\n",
    "# Empty list to store errors for each fold\n",
    "fold_errors = []\n",
    "\n",
    "# 2.2) Initialize 5-Fold Cross-Validation, save the object as CV_kfold\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Loop through the folds\n",
    "for fold, (train_index, test_index) in enumerate(CV_kfold.split(X)):\n",
    "\n",
    "    # 2.3) Split X and y into X_train, X_test, y_train and y_test according to the current fold.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Initialize and fit model.\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train.reshape(-1, 1), y_train)\n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train.reshape(-1, 1))\n",
    "    y_test_pred  = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "    # Compute errors\n",
    "    train_error = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_error  = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    fold_errors.append((train_error, test_error))\n",
    "\n",
    "# Bar plots for each fold train and test error\n",
    "fold_errors = np.array(fold_errors)\n",
    "plot_fold_errors(fold_errors)\n",
    "plt.title('5-fold cross-validation: train vs test error per fold')\n",
    "plt.show()\n",
    "\n",
    "# Save mean errors for later comparison\n",
    "errors['5-Fold CV'] = (np.mean(fold_errors[:, 0]), np.mean(fold_errors[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6ef7a",
   "metadata": {},
   "source": [
    "**Task 2.4:** Initialize the LeaveOneOut method, save it as `CV_loo`.\n",
    "> *Hint:* Use the function `LeaveOneOut()`. \n",
    "\n",
    "> *Hint:* Why do we not need to shuffle the data and set a seed using this method?\n",
    "\n",
    "**Task 2.5:** Split $\\boldsymbol{X}$ and $\\boldsymbol{y}$ into a training and test split within each fold.\n",
    "> *Hint:* Index `X` and `y` by the indexes provided in the for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd0bc11",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fad8bdfb893a3cd793bcb4ad4f5849f",
     "grade": true,
     "grade_id": "cell-750d5d8349c1328f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Empty list to store errors for each fold\n",
    "fold_errors = []\n",
    "\n",
    "# 2.4) Leave-one-out cross-validation\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Loop through the folds\n",
    "for fold, (train_index, test_index) in enumerate(CV_loo.split(X)):\n",
    "    # 2.5) Split X and y into X_train, X_test, y_train and y_test according to the current fold.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Initialize model\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    # Fit model\n",
    "    model.fit(X_train.reshape(-1, 1), y_train)\n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train.reshape(-1, 1))\n",
    "    y_test_pred  = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "    # Compute errors\n",
    "    train_error = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_error  = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    fold_errors.append((train_error, test_error))\n",
    "\n",
    "# Bar plots for each fold train and test error\n",
    "fold_errors = np.array(fold_errors)\n",
    "plot_fold_errors(fold_errors)\n",
    "plt.title('Leave-one-out cross-validation: train vs test error per fold')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save mean errors for later comparison\n",
    "errors['LeaveOneOut CV'] = (np.mean(fold_errors[:, 0]), np.mean(fold_errors[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6042c",
   "metadata": {},
   "source": [
    "**Task 2.6:** How come the test results vary so much from fold to fold in the cases of K-fold and leave-one-out? Relate it to the single fold of holdout, and why these methods are better at estimating the Generalization Error. \n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5da2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of all methods\n",
    "f = plt.figure()\n",
    "plt.title('Comparison of Methods: Average Train vs Test Error')\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(errors))\n",
    "train_errs = [v[0] for v in errors.values()]\n",
    "test_errs  = [v[1] for v in errors.values()]\n",
    "plt.bar(index, train_errs, bar_width, color='tab:blue', label='Train error')\n",
    "plt.bar(index + bar_width, test_errs, bar_width, color='tab:red', label='Test error')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(index + bar_width / 2, [k for k in errors.keys()])\n",
    "plt.ylim(0, max(train_errs + test_errs) + 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6031e2",
   "metadata": {},
   "source": [
    "**Task 2.7:** Which is a more accurate estimation of the Generalization Error?\n",
    "- *Answer:*\n",
    "\n",
    "**Task 2.8:** How come leave-one-out gets a lower test error than 5-fold? Try thinking of how many data points were used when training the models within each fold. \n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ffe46",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Cross-Validation for Model Evaluation\n",
    "\n",
    "When optimizing our models, we'd like to make sure the estimate of our generalization error is as good as possible, so we can make sure we've chosen the best parameters. In this exercise, we will be using leave-one-out CV, to evaluate the performance of KNN models with $K = 1, 2, \\dots, 40$ on the Iris dataset, to find the best performing number of neighbours, $K$.\n",
    "\n",
    "**Task 3.1:** Load the Iris dataset into $\\boldsymbol{X}$ and $\\boldsymbol{y}$.\n",
    "> *Hint:* Remember to make $\\boldsymbol{y}$ categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ffa08",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a0cc98016de9d13bfc41879b0b75daf",
     "grade": true,
     "grade_id": "cell-112eedc1cb08a7ba",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert X.shape == (150, 4), \"There should be 150 samples and 4 features in the Iris dataset.\"\n",
    "assert y.shape == (150,), \"There should be 150 labels in the Iris dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18c43d",
   "metadata": {},
   "source": [
    "**Task 3.2:** Implement leave-one-out cross-validation in the code below. \n",
    "> *Hint:* Use `LeaveOneOut()` for the cross-validation.\n",
    "\n",
    "**Task 3.3:** Split $\\boldsymbol{X}$ and $\\boldsymbol{y}$ according to the current fold in the for-loop.\n",
    "> *Hint:* You might have to use `.iloc` as we're using Pandas for our data.\n",
    "\n",
    "**Task 3.4:** For each fold, loop over the different values of $k$. For each $k$, define a KNN model with $k$ neighbors, fit it, make predictions, and compute the classification error. Store the result in `errors[fold, k - 1]`.\n",
    "> *Hint:* Use `KNeighborsClassifier` from `sklearn.neighbours`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad9961",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3f119683fed11145a19cc9c124d92cc",
     "grade": true,
     "grade_id": "cell-7cc2cebc8ebb3430",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "K_neighbours = 40  # Max neighbors\n",
    "errors = np.zeros((len(X), K_neighbours))\n",
    "\n",
    "# 3.2) Implement leave-one-out cross-validation.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Loop through the folds\n",
    "for fold, (train_index, test_index) in enumerate(CV_loo.split(X)):\n",
    "    # 3.3) Split X and y according to the current fold, name them X_train, X_test, y_train, y_test.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    for k in range(1, K_neighbours + 1):\n",
    "        # 3.4) For each $k$, define a KNN model with $k$ neighbors, fit it, make predictions.\n",
    "        # Compute the classification error. \n",
    "        # Store the result in errors[fold, k - 1]\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Plot mean errors for each K\n",
    "mean_errors = np.mean(errors, axis=0)\n",
    "f = plt.figure()\n",
    "plt.title('Leave-One-Out CV')\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Classification Error (%)')\n",
    "plt.plot(range(1, K_neighbours + 1), mean_errors * 100)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131701a",
   "metadata": {},
   "source": [
    "**Task 3.4:** Argue the number of neighbours that would be optimal for our KNN model on the Iris data. How come the estimated generalization error suddenly rises after the classification error minima?\n",
    "- *Answer:*\n",
    "\n",
    "**Task 3.5:** Try switching out the model for a Decision Tree instead, evaluate the best depth. \n",
    "\n",
    "**Task 3.6:** Try exchanging the LOO for Holdout and 10-fold CV and time the run-time. What's the time difference, and how come?\n",
    "> *Hint:* You might not even have to change the code, can you think of a way to calculate an estimation of the time difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00983370",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 4: Feature Selection through 2-layer cross-validation\n",
    "In this exercise we consider cross-validation for variable selection and model performance evaluation in linear regression. We will try to predict the body-weight of a person, based on a number of body measurements, using linear regression with feature subset selection. \n",
    "\n",
    "The data can be found in the `body.csv` file, and is a subset of the data available [here](http://www.sci.usq.edu.au/courses/STA3301/resources/Data/). To measure how well we can predict the body-weight, we will use the mean squared error (MSE) between the true and estimated body-weight.\n",
    "\n",
    "We will make use of two layers of cross-validation to perform our feature selection. \n",
    "1) In the outer layer, we use 5-fold cross-validation to estimate the performance of our model, i.e., we compute the squared error averaged over 5 outer test sets. \n",
    "2) In the inner layer, we use 10-fold cross-validation to perform sequential feature selection.\n",
    "    - We start by evaluating each feature in the 10 inner folds - the best feature moves on, and we sequentially add more features until no improvement can be found. \n",
    "\n",
    "The best model found within each outer fold, is then finally tested on its corresponding outer test fold. \n",
    "\n",
    "The process is also described in this visualization:\n",
    "<p align=\"center\">\n",
    "  <img src=\"2layerCV.jpg\" alt=\"Two-layer cross-validation\" height=\"400px\">\n",
    "</p>\n",
    "\n",
    "The code will produce 6 plots - the first 5 describe the results of each outer fold. \n",
    "- (Left): The MSE found for each sequentially added feature across the iterations, for the corresponding fold. \n",
    "- (Right): A grid showing which features were used in each iteration, for the corresponding fold. \n",
    "\n",
    "The last plot describes \n",
    "- (Left): The outer test MSE found for the best model, on each fold.\n",
    "- (Right): The best features found for each fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f69e32",
   "metadata": {},
   "source": [
    "**Task 4.1:** Get acquainted with the figure above - make sure you understand how the 2-layer CV works, and what is supposed to happen in the inner and outer folds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2511ccb",
   "metadata": {},
   "source": [
    "**Task 4.2:** Load the `body.csv` into pandas, split into X and y (`weight`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9b4fb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "164ba29036769fef0a96021d46c54bfd",
     "grade": true,
     "grade_id": "cell-d3483a2a12361f76",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert X.shape == (30, 23), \"There should be 30 samples and 23 features in the Body dataset.\"\n",
    "assert y.shape == (30,), \"There should be 30 labels in the Body dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820211d",
   "metadata": {},
   "source": [
    "**Task 4.3:** Split `X` and `y` into an outer training and test split within each outer fold.\n",
    "> *Hint:* Name them `X_train_outer`, `X_test_outer`, `y_train_outer`, `y_test_outer`.\n",
    "\n",
    "> *Hint:* Index by the indices provided in the outer CV-loop - remember to use `.iloc`.\n",
    "\n",
    "**Task 4.4:** Split `X_train_outer` and `y_train_outer` into an inner training and test split within each inner fold.\n",
    "> *Hint:* Name them `X_train_inner`, `X_test_inner`, `y_train_inner`, `y_test_inner`.\n",
    "\n",
    "> *Hint:* Index by the indices provided in the inner CV-loop - remember to use `.iloc`.\n",
    "\n",
    "**Task 4.5:** Fill in the code to create the Linear Regression model, fit, and predict in both the inner and outer fold. \n",
    "> *Hint:* Remember `fit_intercept=True`.\n",
    "\n",
    "> *Hint:* In the inner folds: Only fit and test the model on the `candidate_features`.\n",
    "\n",
    "> *Hint:* In the outer folds: Only fit and test the model on the `selected_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18669db8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b15b8e7cc44054f38d74718fc07261",
     "grade": true,
     "grade_id": "cell-18ee0d599163d902",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sequential_feature_selection(X, y, outer_folds=5, inner_folds=10, random_state=42):\n",
    "    CV_outer = KFold(n_splits=outer_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_results = []  # store per-fold feature paths & errors\n",
    "    outer_test_mse = []\n",
    "\n",
    "    for outer_train_idx, outer_test_idx in CV_outer.split(X):\n",
    "\n",
    "        # 4.3) Split X and y into training/testing data for this outer fold\n",
    "        # Name them X_train_outer, X_test_outer, y_train_outer, y_test_outer\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        selected_features = []\n",
    "        mse_per_iteration = []\n",
    "        remaining_features = list(X.columns)\n",
    "        best_mse = np.inf\n",
    "\n",
    "        # Feature addition while-loop, keep adding features until no improvement\n",
    "        while remaining_features:\n",
    "            feature_mse = {}\n",
    "\n",
    "            for feat in remaining_features:\n",
    "                candidate_features = selected_features + [feat]\n",
    "\n",
    "                # inner CV\n",
    "                CV_inner = KFold(n_splits=inner_folds, shuffle=True, random_state=random_state)\n",
    "                inner_mse = []\n",
    "\n",
    "                for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer):\n",
    "\n",
    "                    # 4.4) Split X_train_outer and y_train_outer into training/testing data for this inner fold.\n",
    "                    # Name them X_train_inner, X_test_inner, y_train_inner, y_test_inner\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "                    \n",
    "                    # 4.5.1) Create Linear Regression model, fit, and predict on the inner sets\n",
    "                    # - save predictions as y_test_inner_pred.\n",
    "                    # Remember to only take the candidate_features into consideration.\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "                    # Add score for this fold\n",
    "                    inner_mse.append(mean_squared_error(y_test_inner, y_test_inner_pred))\n",
    "\n",
    "                # Save average MSE with this feature added\n",
    "                feature_mse[feat] = np.mean(inner_mse)\n",
    "\n",
    "            # pick best new feature\n",
    "            best_feat = min(feature_mse, key=feature_mse.get)\n",
    "            best_feat_mse = feature_mse[best_feat]\n",
    "\n",
    "            # If best feature improves MSE, add it\n",
    "            if best_feat_mse < best_mse:\n",
    "                selected_features.append(best_feat)\n",
    "                mse_per_iteration.append(best_feat_mse)\n",
    "                best_mse = best_feat_mse\n",
    "                remaining_features.remove(best_feat)\n",
    "\n",
    "            else: # No improvement - get actual test MSE for outer loop, with best feature set.\n",
    "                \n",
    "                # 4.5.2) Create Linear Regression model, fit, and predict on the outer sets\n",
    "                # - save predictions as y_test_outer_pred.\n",
    "                # Remember to only take the selected_features into consideration.\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "\n",
    "                outer_test_mse = mean_squared_error(y_test_outer, y_test_outer_pred)\n",
    "                break # Break out of while loop\n",
    "\n",
    "        fold_results.append({\n",
    "            \"mse\": mse_per_iteration,\n",
    "            \"features\": selected_features,\n",
    "            \"outer_test_mse\": outer_test_mse\n",
    "        })\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "\n",
    "results = sequential_feature_selection(X, y, random_state=1)\n",
    "plot_seq_feature_results(results, list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729a9c4",
   "metadata": {},
   "source": [
    "**Task 4.5:** The MSE for the 2nd outer fold is much higher than the rest, why might this be? Try relating it to why K-fold is typically better than Holdout at estimating the Generalization Error. \n",
    "\n",
    "- *Answer:*\n",
    "\n",
    "**Task 4.6:** Explain how it can be seen the feature selection tends to choose features such as height and waist girth, and disregard features such as the wrist diameter, which seems reasonable when predicting body-weight.\n",
    "\n",
    "- *Answer:*\n",
    "\n",
    "**(Optional) Task 4.7:** The above code does *forward* sequential feature selection. Try copy-pasting the code into a cell below, and modify it such that it performs *backwards* feature selection instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d308e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02450-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
