{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcdd60e",
   "metadata": {},
   "source": [
    "**02452** *Machine Learning*, Technical University of Denmark\n",
    "\n",
    "- This Jupyter notebook contains exercises where you fill in missing code related to the lecture topic. *First*, try solving each task yourself. *Then* use the provided solution (an HTML file you can open in any web browser) as inspiration if needed. If you get stuck, ask a TA for help.\n",
    "\n",
    "- Some tasks may be difficult or time-consuming - using the solution file or TA support is expected and perfectly fine, as long as you stay active and reflect on the solution.\n",
    "\n",
    "- You are not expected to finish everything during the session. Prepare by looking at the exercises *before* the class, consult the TAs *during* class, and complete the remaining parts *at home*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44baaabf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c6467",
   "metadata": {},
   "source": [
    "# Week 3: Computational Linear Algebra and Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7dede",
   "metadata": {},
   "source": [
    "**Content:**\n",
    "- Part 1: Understanding the basics of PCA\n",
    "- Part 2: PCA on the NanoNose dataset\n",
    "- Part 3: Hidden structure in handwritten digits\n",
    "- Part 4: $k$-nearest neighbors on reduced data\n",
    "\n",
    "**Objectives:**\n",
    "- Get acquainted with how data can be filtered and visualized using principal component analysis (PCA).\n",
    "- Can apply and interpret principal component analysis (PCA) for data visualization and dimensionality reduction / feature extraction.\n",
    "- Understand and apply matrix operations on data matrices with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf34579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20399509",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data in the real world is typically **high-dimensional**, containing many collected attributes - as examples, you have already worked with images and the wine dataset. According to the **[manifold hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis?utm_source=chatgpt.com)**, much of this data actually lies on a lower-dimensional manifold, meaning that the data can be explained in fewer dimensions - i.e. there is an **underlying structure that can be described with fewer dimensions**. Exploiting this structure allows us to remove redundant information in the data which in turn can help reduce computational and storage requirements as well as make high-dimensional data easier to visualize and interpret.\n",
    "\n",
    "Using basic linear algebra, we can project data from its original high-dimensional space into a **lower-dimensional subspace**. The central question then becomes: what projection best preserves the structure of the data? In this week’s exercise, we explore one of the most widely used approaches to answering this question, namely Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa660c4",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the basics of PCA\n",
    "\n",
    "In this exercise, we will build intuition for PCA by constructing a simple toy example, recreating the PCA example of a Gaussian point cloud from the lecture slides. \n",
    "\n",
    "For intuition, we first generate a \"high\"-dimensional dataset that lie in a lower-dimensional space by:\n",
    "1) generating $N=1000$ samples in $\\mathbb{R}^2$ from a multivariate Gaussian distribution (more on that next week!). We store these points as $\\boldsymbol{Z}$ of shape $N \\times K$.\n",
    "2) embedding these points linearly in $\\mathbb{R}^3$. We store the embedded points in our well-known data matrix $\\boldsymbol{X}$ of shape $N \\times M$. \n",
    "\n",
    "We provide the code and visualize the point cloud in the cell below.\n",
    "\n",
    "\n",
    "**Task 1.1:** Verify that the points in $\\mathbb{R}^3$ actually lie on a 2-dimensional plane.\n",
    "\n",
    "> *Hint:* Remember that you can add the magic `%matplotlib qt` in the beginning of the cell to open the plot in a separate window where you can interact with the 3D element. Remember to also revert to `%matplotlib inline` when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "N = 1000\n",
    "M = 3\n",
    "K = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the mean and covariance matrix for the 2D Gaussian\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[0.5, 0.5], [0.5, 1.0]])\n",
    "\n",
    "# Sample points from the 2D Gaussian\n",
    "Z = np.random.multivariate_normal(mu, Sigma, N)\n",
    "\n",
    "# Embed the points in the higher-dimensional space\n",
    "X = np.hstack((Z, (Z[:, 0] + Z[:, 1])[:, np.newaxis])) # Add a third dimension as a linear combination of the first two\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Data in lower-dimensional space')\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c='lightsteelblue', marker='.')\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.view_init(elev=10, azim=-155)\n",
    "ax.set_title('Data in higher-dimensional space')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c='lightsteelblue', marker='.')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b9cd4",
   "metadata": {},
   "source": [
    "PCA is a linear projection technique for reducing dimensionality by finding the directions (principal components) along which the data exhibits the **greatest variance**, thereby **maximizing separation** between points in the projected subspace. As seen in the lecture, identifying these principal components $\\left[\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_M\\right]$ reduces to solving an eigenvalue problem that optimizes for maximal variance:\n",
    "$$\n",
    "    \\underset{\\boldsymbol{v}}{\\arg \\max} \\ \\text{Var}[\\boldsymbol{X} \\boldsymbol{v}] \\quad\\text{s.t.}\\quad \\lVert\\boldsymbol{v}\\rVert^2 = 1 \\qquad \\Rightarrow \\qquad\n",
    "    \\frac{1}{N-1} \\tilde{\\boldsymbol{X}}^\\top \\tilde{\\boldsymbol{X}} \\boldsymbol{v}_i = \\lambda \\boldsymbol{v}_i\n",
    "$$ \n",
    "where $\\tilde{\\boldsymbol{X}} = \\boldsymbol{X} - \\boldsymbol{\\mu}$ is the centered data (i.e. it has zero mean) and hence $\\hat{\\boldsymbol{S}}=\\frac{1}{N-1} \\tilde{\\boldsymbol{X}}^\\top \\tilde{\\boldsymbol{X}}$ is the estimated correlation matrix of the data. We can therefore find the PCs as the eigenvectors of $\\hat{\\boldsymbol{S}}$ which we do efficiently using singular value decomposition (SVD) on the zero-mean data matrix, i.e. $\\tilde{\\boldsymbol{X}} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^T$, resulting in:\n",
    "$$\n",
    "    \\boldsymbol{V}_M = \\begin{bmatrix} | & | & & | \\\\ \\boldsymbol{v}_1 & \\boldsymbol{v}_2 & \\dots & \\boldsymbol{v}_M \\\\ | & | & & | \\end{bmatrix} \\qquad \\text{and} \\qquad \\boldsymbol{\\Sigma} = \\begin{bmatrix} \\sigma_1 & 0 & \\dots & 0 \\\\ 0 & \\sigma_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\sigma_M\\end{bmatrix}\n",
    "$$\n",
    "where $\\sigma_i$ is the $i$'th singular value. Using this, we can find out how much of the variation in the data each PCA component accounts for by computing the eigenvalue. We call this the **explained variance** and compute it as\n",
    "$$\n",
    "    \\lambda_i = \\frac{1}{(N-1)} \\sigma^2_{i}\n",
    "$$\n",
    "Similarly, the fraction of explained variance for the $i$'th component is\n",
    "$$\n",
    "  \\rho_i = \\frac{\\lambda_i}{\\sum_{j=1}^M \\lambda_j} = \\frac{\\frac{1}{(N-1)} \\sigma^2_{i}}{\\sum_{j=1}^M \\frac{1}{(N-1)} \\sigma^2_{i}} = \\frac{\\sigma_i^2}{\\sum_{j=1}^M \\sigma_j^2}\n",
    "$$\n",
    "i.e. the squared singular value of the given component divided by the sum of all the squared singular values. This equation is critical to understanding PCA and we encouge you to study it closer in the lecture notes in Chapter 3.\n",
    "\n",
    "Lastly, remember that projections $\\boldsymbol{b}_i$ and reconstructions $\\hat{\\boldsymbol{x}}_i$ can be computed for a collection of data points as:\n",
    "$$\n",
    "    \\boldsymbol{B} = \\tilde{\\boldsymbol{X}} \\boldsymbol{V}_{K} \\quad \\text{and} \\quad \\hat{\\boldsymbol{X}} = \\boldsymbol{V}_K \\boldsymbol{B}\n",
    "$$\n",
    "\n",
    "\n",
    "As you can see, PCA only considers finding structure in observed data without knowing any target attribute and is therefore an **unsupervised learning** technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff3f00",
   "metadata": {},
   "source": [
    "**Task 1.2:** Compute the PCA of the example above. Project the observations $\\boldsymbol{X}$ to a lower dimensional subspace using $\\boldsymbol{V}_K$ where $K=2$ and plot them as a scatter plot. Plot the $K=2$ principle components / directions on top of 1) the 3D point cloud and 2) the projected data.\n",
    "\n",
    "> *Hint:* Use `sklearn.decomposition.PCA` to compute the PCA. Checkout the documentation for fitting and transforming/projecting with the PCA, use `fit_transform`\n",
    "\n",
    "> *Hint:* Extract the principal components, i.e. $\\boldsymbol{V}$, with the `components_` attribute of the PCA class. Are they the correct shape? We want each PC to be a column.\n",
    "\n",
    "> *Hint:* Simply plot the PCs as lines using `plt.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df43e76",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93effc124f577614d4217c0d68bb02cd",
     "grade": false,
     "grade_id": "cell-a74f0b9cd688c295",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create PCA object\n",
    "pca = PCA(n_components=M)\n",
    "\n",
    "# Fit and transform the data to get B\n",
    "# Get the principal components, V_K\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Define colors for the principal components\n",
    "colors = ['red', 'darkgreen', 'orange']\n",
    "\n",
    "# Create figure and subplot for high-dimensional data\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_title('Data in higher-dimensional space')\n",
    "ax.view_init(elev=10, azim=-155) # viewing parameters\n",
    "\n",
    "# Plot the high dimensional data, X - and the principal components, V_K, as lines. \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Figure layout\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$z$')\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "\n",
    "# Create subplot for projected data\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Data projected to lower-dimensional space')\n",
    "\n",
    "# Plot the projected data, B, in 2D - along with the principal components in the projected space (which are just the standard basis vectors in 2D).\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Figure layout\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327ef8f",
   "metadata": {},
   "source": [
    "**Task 1.3:** Compute the explained variance and the fraction of explained variance for all $M=3$ components. Make two plots showing 1) a bar plot of explained variance per component and 2) the accumulated fraction of explained variance when increasing the number of principle components.\n",
    "\n",
    "> *Hint:* Use the method `.explained_variance_` to get the explained variance per component.\n",
    "\n",
    "> *Hint:* Use the method `.explained_variance_ratio` to get the fraction of explained variance per component.\n",
    "\n",
    "> *Hint:* Use `np.cumsum()` to accumulate values of a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9743d32",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "041272675260f559bc0a236d65069131",
     "grade": false,
     "grade_id": "cell-8c8c7023d7999572",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Explained variance plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].set_title('Explained Variance')\n",
    "axs[0].set_xlabel('Principal Components')\n",
    "axs[0].set_ylabel('Explained Variance')\n",
    "\n",
    "# Bar plot for explained variance\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Accumulated explained variance plot\n",
    "axs[1].set_title('Accumulated fraction of explained variance')\n",
    "axs[1].set_xlabel('Principal Components')\n",
    "axs[1].set_ylabel('Accumulated Explained Variance')\n",
    "\n",
    "# Line plot for accumulated explained variance\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ca9c6",
   "metadata": {},
   "source": [
    "As you will be able to tell from the plot, the first two principal components are able to explain **ALL** the variance of our 3-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74679db5",
   "metadata": {},
   "source": [
    "**Task 1.4:** Using the explained variance figures, argue how we can see that all the observed data from $\\mathbb{R}^3$ lie on a lower-dimensional subspace in $\\mathbb{R}^2$.\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a556a0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 2: PCA on the NanoNose dataset\n",
    "\n",
    "As an example dataset we will consider chemical sensor data obtained from the NanoNose project. The data contains 8 sensors named by the letters $A$-$H$ measuring different levels of concentration of Water, Ethanol, Acetone, Heptane and Pentanol injected into a small gas chamber. The data will be represented in matrix form such that each row contains the 8 sensors measurements (i.e. sensor $A$-$H$) of the various compounds injected into the gas chamber."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c4e79",
   "metadata": {},
   "source": [
    "\n",
    "**Task 2.1:** Inspect the `nanonose.xls` file from the associated data folder and consider potential issues to be aware of when loading the file. Load the data and construct a data matrix $\\boldsymbol{X}$ containing attributes $A$-$H$ as well as a descriptive target attribute $\\boldsymbol{y}$ containing the fluid type as numerical values.\n",
    "\n",
    "> *Hint:* Use `pd.read_excel()` to load `.xls`-files.\n",
    "\n",
    "> *Hint:* Use indexing to choose the correct rows/columns when constructing $\\boldsymbol{X}$ and $\\boldsymbol{y}$.\n",
    "\n",
    "> *Hint:* Use `sklearn.preprocessing.LabelEncoder` to easily construct $\\boldsymbol{y}$ as a numerical array. This can be done by defining `label_encoder = LabelEncoder()` and mapping the target with `y = label_encoder.fit_transform(df.loc[1:, \"Nanonose\"])`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc12f2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e8bc588aa699a4e93235976e7a3778b",
     "grade": false,
     "grade_id": "cell-64d2a50a9f8d2fe3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the NanoNose dataset into X and y\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the data shape\n",
    "assert X.shape == (90, 8), \"There should be 90 samples and 8 features\"\n",
    "assert y.shape == (90,), \"There should be 90 samples in the target variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da6789",
   "metadata": {},
   "source": [
    "The data resides in an 8-dimensional space where each dimension corresponds to each of the 8 NanoNose sensors. This makes visualization of the raw data difficult, because it is difficult to plot data in more than 2-3 dimensions.\n",
    "\n",
    "**Task 2.2:**  Plot the two attributes $A$ and $B$ against each other in a scatter plot and color by the liquid type, `y`. What do you see? Try to change the dimensions that are plotted against each other.\n",
    "\n",
    "> *Hint:* Remember that you can use the inherent plotting functionalities of Pandas dataframes! `X.plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9833a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbabb33cf6b7010145a3537165313184",
     "grade": false,
     "grade_id": "cell-d416b9687f0b0436",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68145d2e",
   "metadata": {},
   "source": [
    "**Task 2.3:** Compute the PCA of the NanoNose data and plot the percent of variance explained by the principal components as well as the cumulative percentage of variance explained. \n",
    "\n",
    "> Can you verify that more than $90\\%$ of the variation in the data is explained by the first 3 principal components? How many components would be needed for 95%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536d3bb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "281381591ce12ecb3d053d4274e3ffca",
     "grade": false,
     "grade_id": "cell-1954db41f336bcf5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create PCA object and fit to the data\n",
    "# Extract the principal components, V, and the variance explained ratio, rho\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# 90% threshold for variance explained\n",
    "threshold = 0.9\n",
    "\n",
    "# Plot variance explained\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(rho) + 1), rho, \"x-\")\n",
    "plt.plot(range(1, len(rho) + 1), np.cumsum(rho), \"o-\")\n",
    "plt.plot([1, len(rho)], [threshold, threshold], \"k--\")\n",
    "plt.title(\"Variance explained by principal components\")\n",
    "plt.xlabel(\"Principal component\")\n",
    "plt.ylabel(\"Variance explained\")\n",
    "plt.legend([\"Individual\", \"Cumulative\", \"Threshold\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e582e97e",
   "metadata": {},
   "source": [
    "**Task 2.4** Project the data onto principal component 1 and 2 and plot these against each other in a scatterplot. What are the benefits of visualizing the data by the projection given by PCA over plotting two of the original data dimensions against each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a093b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3c7cf18d1f567457311507b0e46ea34",
     "grade": false,
     "grade_id": "cell-df89b6e147f7f440",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PC_idxs = [0, 1]  # Indices of the principal components to plot\n",
    "unique_classes = np.unique(y) # Get unique classes from the target variable\n",
    "\n",
    "# Project the data onto the first two principal components and plot, colored by their fluid type.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26879318",
   "metadata": {},
   "source": [
    "As your previous result should reveal, the first 3 components explaine more than $90$ percent of the variance. To better understand what they tell us about the data, we will take a look at their coefficients.\n",
    "\n",
    "**Task 2.5:** Interpret the $K=3$ first principal directions from $\\boldsymbol{V}$ obtained using the PCA by plotting the coefficients for each attribute as bars in a bar plot. \n",
    "\n",
    "> *Hint:* \n",
    "\n",
    "**Task 2.6:** Which of the original attributes does the second principal component mainly capture the variation of and what would cause an observation to have a large negative/positive projection onto the second principal component?\n",
    "\n",
    "> *Hint:* remember that both the attributes and the prinpal component has a sign and a magnitude.\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc055c8e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7394256e76bc0a0bc3306bf68874e9c",
     "grade": false,
     "grade_id": "cell-770acef5688f65c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4466638",
   "metadata": {},
   "source": [
    "**Task 2.7:** Extract all observations taken of \"Water\". Project the water data onto the 2nd principal component. Based on the coefficients and the attribute values for these observations, would you expect the projection onto PC2 to be positive or negative - why? \n",
    "\n",
    "> *Hint:* Create a mask that finds observations where `y` corresponds to the water class. Then use `.iloc` to index the `X` dataframe.\n",
    "\n",
    "> *Hint:* Consider *both* the magnitude and sign of *both* the coefficient and the attribute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8af071",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e600ac133ee62d321565fe497f29803e",
     "grade": false,
     "grade_id": "cell-9a9fce5c38d822a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Projection of water class onto the 2nd principal component.\n",
    "\n",
    "# As seen from the figure above, the attribute A and E are negatively \n",
    "# projected onto PC2 while H is positively projected. When looking at \n",
    "# the observations of water, we see that A and E attributes holds large \n",
    "# positive values while H contains low positive values. For determining\n",
    "# the sign of the projection on PC2, H therefore has a negligible positive\n",
    "# contribution. We can also see it by actually projecting the data onto \n",
    "# the 2nd principal component.\n",
    "\n",
    "# 1) Extract the water data\n",
    "# 2) Project the water data onto the 2nd principal component\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Print statistics of the projection\n",
    "print(f\"All water data:\\n{all_water_data.head()}\\n\")\n",
    "print(f\"Statistics of water data projected onto PC2:\\n{water_data_pc2.describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e479a6",
   "metadata": {},
   "source": [
    "Another way to approach interpreting the principal directions is to plot the coefficients as vectors in the principal component space. In the PC1/PC2-space, we can for instance interpret the relationship between PC1, PC2 and a given attribute by drawing a line form Origo to the coefficients in PC1 and PC2 corresponding to the attribute. The direction and magnitude of such a vector defines how the data from that attribute is projected onto the PC1/PC2-space - e.g. if the vector points in positive direction of PC1, then positive values of that attribute contributes to a positive projection onto PC1. Since the vectors in $\\boldsymbol{V}$ are unit-vectors, all coefficients will lie within the unit-circle.\n",
    "\n",
    "**Task 2.8:** Plot the dataset in the PC1/PC2 space alongside the accumulated explained variance ratio as well as the attribute coefficient vectors from the PCA loadings, showing their direction and magnitude within the unit circle to interpret how each attribute contributes to the principal components.\n",
    "\n",
    "> *Hint:* Use `plt.arrow()` from to plot coefficient vectors from the origin to the (PC1, PC2) loadings for each attribute.\n",
    "\n",
    "> *Hint:* Add the attribute name to the arrows using `plt.text()`.\n",
    "\n",
    "> *Hint:* Draw a unit circle to show that loadings are bounded by magnitude 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56bd940",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80671a4f0c943fd198318dae1f6eb78f",
     "grade": false,
     "grade_id": "cell-a87204fa9aeaa04b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PC_idxs = [0, 1]  # Indices of the principal components to plot\n",
    "\n",
    "# Fit and transform the data into B, extract the principal components, V\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Make the plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# Plot the data projected onto PC1/PC2\n",
    "axs[0].set_title(\"NanoNose data: PCA\")\n",
    "for fluid_type in unique_classes:\n",
    "    mask = (y == fluid_type)\n",
    "    axs[0].plot(B[mask, PC_idxs[0]], B[mask, PC_idxs[1]], \".\", alpha=0.5, label=fluid_type)\n",
    "\n",
    "axs[0].set_xlabel(f\"PC{PC_idxs[0] + 1}\")\n",
    "axs[0].set_ylabel(f\"PC{PC_idxs[1] + 1}\")\n",
    "axs[0].axis(\"equal\")\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "axs[1].set_title(\"Accumulated explained variance ratio\")\n",
    "axs[1].plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_.cumsum())\n",
    "axs[1].set_xlabel(\"Principal component\")\n",
    "axs[1].set_ylabel(\"Explained variance ratio\")\n",
    "\n",
    "# Plot attribute coefficients in principal component space\n",
    "axs[2].set_title(\"Attribute coefficients in PC space\")\n",
    "for attr_idx, attr_name in enumerate(X.columns):\n",
    "    # Plot an arrow for each attribute, add a label\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "axs[2].set_xlim([-1, 1])\n",
    "axs[2].set_ylim([-1, 1])\n",
    "axs[2].set_xlabel(f\"PC{PC_idxs[0] + 1}\")\n",
    "axs[2].set_ylabel(f\"PC{PC_idxs[1] + 1}\")\n",
    "axs[2].grid()\n",
    "# Add a unit circle\n",
    "axs[2].plot(np.cos(np.arange(0, 2 * np.pi, 0.01)), np.sin(np.arange(0, 2 * np.pi, 0.01)))\n",
    "axs[2].axis(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d29f6",
   "metadata": {},
   "source": [
    "We can correct for differences in scale by standardization. When doing PCA on data with attributes of different scales, it can be very important to standardize the dataset. We standardize a dataset by ensuring each attribute has a mean of zero (as before), but also has a variance of one (i.e. zero mean and unit variance). \n",
    "\n",
    "**Task 2.9:** Compute the standard deviation of each attribute in the NanoNose dataset. Are the attributes on similar scale or should we standardize the data?\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093509c5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6ac28827bb43f20777805f0e17c3855",
     "grade": false,
     "grade_id": "cell-20aaaabd836c946f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute standard deviation of each attribute in X\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206cc488",
   "metadata": {},
   "source": [
    "**Task 2.10:** Standardize the dataset and recompute the PCA. Plot the same figure as in Task 2.8 and explain the difference between centering (zero-mean) or standardizing (zero-mean and unit variance) the data. How did the attribute with the highest standard deviation change in terms of its direction and magnitude in the attribute coefficients? How did the variance explained change? \n",
    "\n",
    "- *Answer:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b6eaa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2897760adcc0a3055e4ff86fd89fcec2",
     "grade": false,
     "grade_id": "cell-fb2fd64bd7c72066",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PC_idxs = [0, 1]  # Indices of the principal components to plot\n",
    "\n",
    "# Standardize data, X_tilde, then fit and transform the data into B, extract the principal components, V\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Make the plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# Plot the data projected onto PC1/PC2\n",
    "axs[0].set_title(\"NanoNose data: PCA\")\n",
    "for fluid_type in unique_classes:\n",
    "    mask = (y == fluid_type)\n",
    "    axs[0].plot(B[mask, PC_idxs[0]], B[mask, PC_idxs[1]], \".\", alpha=0.5, label=fluid_type)\n",
    "\n",
    "axs[0].set_xlabel(f\"PC{PC_idxs[0] + 1}\")\n",
    "axs[0].set_ylabel(f\"PC{PC_idxs[1] + 1}\")\n",
    "axs[0].axis(\"equal\")\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "axs[1].set_title(\"Accumulated explained variance ratio\")\n",
    "axs[1].plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_.cumsum())\n",
    "axs[1].set_xlabel(\"Principal component\")\n",
    "axs[1].set_ylabel(\"Explained variance ratio\")\n",
    "\n",
    "# Plot attribute coefficients in principal component space\n",
    "axs[2].set_title(\"Attribute coefficients in PC space\")\n",
    "for attr_idx, attr_name in enumerate(X.columns):\n",
    "    # Plot an arrow for each attribute, add a label (Same solution as before)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "axs[2].set_xlim([-1, 1])\n",
    "axs[2].set_ylim([-1, 1])\n",
    "axs[2].set_xlabel(f\"PC{PC_idxs[0] + 1}\")\n",
    "axs[2].set_ylabel(f\"PC{PC_idxs[1] + 1}\")\n",
    "axs[2].grid()\n",
    "# Add a unit circle\n",
    "axs[2].plot(np.cos(np.arange(0, 2 * np.pi, 0.01)), np.sin(np.arange(0, 2 * np.pi, 0.01)))\n",
    "axs[2].axis(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3f3b2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 3: Hidden structure in handwritten digits\n",
    "\n",
    "The US Postal Service (USPS) wanted to automate the process of sorting letters based on their zip-codes. We will presently consider the dataset of USPS handwritten \n",
    "digits that we also worked with last week for information retrieval. Recall that the images are $16\\times 16$ pixel images stored as $256$-dimensional (flattened) arrays. We have created a training and test set split of the data and provided it in the associated data folder.\n",
    "\n",
    "**Task 3.1:** Load the training and test set splits of the digits dataset from the associated data folder. Construct the data matrix $\\boldsymbol{X}$ and the target attribute $\\boldsymbol{y}$ from the files.\n",
    "\n",
    "> *Hint:* Use `np.load()` to load the `.npy`-files. \n",
    "\n",
    "> *Hint:* The loaded data is of shape $N \\times 257$ - which column corresponds to the label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f4ba5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfb072ec97a65c4d6118c7459610c0cc",
     "grade": false,
     "grade_id": "cell-db60ca73b5edaf4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the data\n",
    "assert X_train.shape == (7291, 256), \"Training data should have 7291 samples and 256 features\"\n",
    "assert y_train.shape == (7291,), \"Training labels should have 7291 samples\"\n",
    "assert X_test.shape == (2007, 256), \"Test data should have 2007 samples and 256 features\"\n",
    "assert y_test.shape == (2007,), \"Test labels should have 2007 samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34aaa0",
   "metadata": {},
   "source": [
    "**Task 3.2:** Visualize 3 images of each digit from the training set in a subplot.\n",
    "> *Hint:* Iterate through the unique digits, and filter the data with a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9c6ab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eea782def5f4293d682c3ebde042722f",
     "grade": false,
     "grade_id": "cell-4d32319f0798ebdb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Number of images per digit\n",
    "show_n_digits = 3\n",
    "# Get the list of unique digits\n",
    "unique_digits = np.unique(y_train)\n",
    "\n",
    "# Setup figure\n",
    "fig, axs = plt.subplots(show_n_digits, len(unique_digits), figsize=(len(unique_digits)*2, show_n_digits*2))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5771b01",
   "metadata": {},
   "source": [
    "The images live in a $M = 256$ dimensional vector space, where each dimension corresponds to a pixel. With PCA, we can project this high-dimensional data into a lower-dimensional subspace that preserves most of the variance and allows us to more easily compare the images while also removing potential noise in the handwritten digits.\n",
    "\n",
    "**Task 3.3:** Compute a PCA on the training data. Show that it requires $K=22$ PCA components to account for more than $90$% of the variance in the data. Plot an explained variance vs. number of components plot, like you've done before.\n",
    "\n",
    "> *Hint:* Plot the explained variance ratio and a 90% threshold.\n",
    "\n",
    "> *Hint:* Create a boolean array, that determines whether the cumulative variance is above 90%.\n",
    "\n",
    "> *Hint:* You can find the index of the first true element of a boolean array with `np.argmax()`, e.g. `np.array([False, False, True, False]).argmax()` will return `2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1ea90",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e104a8101d20bfc30882233a1096229",
     "grade": false,
     "grade_id": "cell-6a88ef76f17471ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Number of components required to reach {threshold:.0%} explained variance: {required_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b8f2f",
   "metadata": {},
   "source": [
    "**Task 3.4:** Show that the first principal component is almost sufficient to separate zeros and ones. Examine the first principal component, discuss and understand what it captures.\n",
    "\n",
    "> *Hint:* Consider restricting the analysis to only training images of zeros and ones for this analysis. Then project these points to the subspace spanned by PC1 and PC2.\n",
    "\n",
    "> *Hint:* Color the projected points according to their target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19317b17",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b5c9bc9cb9178da27ef1ba41b957edc",
     "grade": false,
     "grade_id": "cell-8cd6acbd2892c929",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Which digits to include in the analysis\n",
    "digits_to_include = [0, 1] # or range(10) for all\n",
    "\n",
    "# Project the data onto the principal components and plot the projected data for the selected digits.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841ed2e",
   "metadata": {},
   "source": [
    "As we did for the NanoNose dataset, we can try to interpret what information of the data the different principle components capture, yet with the slight difference that we can now visualize our principle components directly as images.\n",
    "\n",
    "**Task 3.5:** Visualize the principle components that capture $90$% of the variation in data as images.\n",
    "\n",
    "> *Hint:* Nothing fancy here - you have done all the required steps before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb21c8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa5fa739dd5cffc8208b966e7b5e490c",
     "grade": false,
     "grade_id": "cell-35e700a2e1ded2c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup a figure with 10 images per row and adjustable number of rows, depending on required components\n",
    "n_cols = 10\n",
    "n_rows = (required_components // n_cols) + 1\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(2*n_cols, 2*n_rows))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ca15c",
   "metadata": {},
   "source": [
    "Last but not least, we want to see what actually happens to our data if we get rid of say the $10$% information deemed least important by the PCA. Hence, we will not need to **reconstruct** data after removing the information captured by the least important principle components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfccda2",
   "metadata": {},
   "source": [
    "**Task 3.6:** Reconstruct the 3 images per digit that you previously plotted, without considering the $10$% least important directions in the data. Do you see any differences?\n",
    "\n",
    "> *Hint:* You need to project the data onto the $90$% most important components - consider creating a new `PCA` object, with `n_components = required_components`.\n",
    "\n",
    "> *Hint:* To compute the reconstruction with `sklearn` you can use the `.inverse_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26473463",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87271a05973cb6f546f38149ef94e108",
     "grade": false,
     "grade_id": "cell-44405a8c6efed39f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634185b",
   "metadata": {},
   "source": [
    "**Task 3.7:** Change the value of $K$ and show that reconstruction accuracy improves when more principal components are used. How many principal components do you need to be able to see the different digits properly? What happens if you set $K=256$?\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ab683",
   "metadata": {},
   "source": [
    "**Task 3.8:** Try decomposing one digit at a time. Plot the $K=10$ first principal components and explain what happens to the principal components when only a single digit type is analyzed compared to when all digit types are analyzed at the same time. What does the principle components now reflect?\n",
    "\n",
    "> *Hint:* The principle components reflect variation in the data. Before our data was from several digits with variation, now it's only of a single digit at a time.\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471125ce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "888d72416d9abc79baf7c1c0dcf23737",
     "grade": false,
     "grade_id": "cell-faa223f2131a11b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "digit_to_analyze = 0 # digit to consider\n",
    "K = 10 # number of components to plot\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5d6d9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 4: $k$-nearest neighbors on reduced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a665ed8",
   "metadata": {},
   "source": [
    "Last week, we classified data using the $k$-nearest neighbors method, and we will now evaluate how well we can determine the digit class when representing each image by only the first $K$ principal components.\n",
    "\n",
    "There are several motivations for doing this:\n",
    "1.\tRemoving the least informative components can be thought of as filtering out noise, focusing on the main variation in the data.\n",
    "2.\tDistance computations become more expensive as dimensionality increases, so reducing dimensions speeds up training and prediction.\n",
    "3.\tAs we will see next week, the “curse of dimensionality” implies that in very high dimensions, distance measures become less meaningful, reducing classification performance.\n",
    "\n",
    "**Task 4.1:** Using the `sklearn` toolbox, determine how well a $k$-nearest neighbors classifier performs on the test set when trained on the training data projected to $K$ dimensions using PCA. Concretely, for each value $K$, follow these steps:\n",
    "1) Define the PCA object from `sklearn.decomposition.PCA` with $K$ components.\n",
    "2) Fit it to the training data and project the training and test data to the $K$-dimensional subspace using the `.transform()` method to get $\\boldsymbol{B}_{\\text{train}}$ and $\\boldsymbol{B}_{\\text{test}}$. \n",
    "3) Define a $1$-nearest neighbors classifier using `sklearn.neighbors.KNeighborsClassifier` and fit it to the projected training data.\n",
    "4) Predict the test set labels $\\hat{\\boldsymbol{y}}_{\\text{test}}$ using $\\boldsymbol{B}_{\\text{test}}$ and compute the test set error rate.\n",
    "5) Repeat for selected values of $K \\in \\{1, \\dots 256\\}$ and plot the accuracy as a function of the number of components $K$. Report the optimal dimensionality $K^\\ast$ that obtains the lowest error rate on the test set - *note: it should be somewhere between 40-60*.\n",
    "\n",
    "> *Hint:* Place the results in a python dictionary, to make the plot work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b298b4a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd203bc94da63bb8b13dc4132a4f64da",
     "grade": false,
     "grade_id": "cell-4a32e3b32a8b2c93",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# We use tqdm to get a progress bar when doing a for-loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Shape of the data\n",
    "N, M = X_train.shape\n",
    "# Number of experiments to run\n",
    "n_experiments = 50\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f'K*: {K_star}, Error rate: {results[K_star]:.4f}%')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.plot(K_star, results[K_star], 'ro', label=rf'$K^\\ast$ = {K_star}') # Plot best K\n",
    "plt.title('$k$NN classification with PC components')\n",
    "plt.xlabel('Number of PCA Components (K)')\n",
    "plt.ylabel('Test Error Rate (%)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf97405",
   "metadata": {},
   "source": [
    "Lastly, let's see how reducing the number of components impacts the training and inference time for the $k$-nearest neighbors classifier.\n",
    "\n",
    "**Task 4.2:** Using the magic command `%%timeit`, get the average and standard deviation when training the classifier with $K^\\star$ attributes and with $M=256$ attributes. Is the speed-up significant when using $K^\\ast$ components instead of the full dimensionality?\n",
    "\n",
    "> *Hint:* See the first line of the code cells below - this is all we need to do to time the runtime of our code. For more information, check the documentation.\n",
    "\n",
    "> *Hint:* In the two cells, you need to define a $k$NN classifier, fit it to the respective training data and predict on the respective test data.\n",
    "\n",
    "> *Hint:* It should take no more than $\\approx 30$ seconds to run each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ed4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we project training and test data to lower-dimensional space in a separate cell\n",
    "pca = PCA(n_components=K_star)  # Here, K_star should be the optimal number of PCA components found in the previous task\n",
    "B_train = pca.fit_transform(X_train)\n",
    "B_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641f727",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40f5a6c3bb733cfb01199d43a5c3b387",
     "grade": false,
     "grade_id": "cell-6cf5aad40e7f8fb8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "\n",
    "# Implement KNN classifier on PCA-reduced data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4370e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a63036390dd9f35f3bf6f6ced9c1876",
     "grade": false,
     "grade_id": "cell-0c4d13439a27bcf5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -n 100 -r 5\n",
    "\n",
    "# Implement KNN classifier on full data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02450-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
